[
  {
    "objectID": "licence.html",
    "href": "licence.html",
    "title": "1 Open Government Licence v3",
    "section": "",
    "text": "https://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/\nYou are encouraged to use and re-use the Information that is available under this licence freely and flexibly, with only a few conditions.\n\n\nUse of copyright and database right material expressly made available under this licence (the ‘Information’) indicates your acceptance of the terms and conditions below.\nThe Licensor grants you a worldwide, royalty-free, perpetual, non-exclusive licence to use the Information subject to the conditions below.\nThis licence does not affect your freedom under fair dealing or fair use or any other copyright or database right exceptions and limitations.\nYou are free to:\n\ncopy, publish, distribute and transmit the Information;\nadapt the Information;\nexploit the Information commercially and non-commercially for example, by combining it with other Information, or by including it in your own product or application.\n\nYou must (where you do any of the above):\n\nacknowledge the source of the Information in your product or application by including or linking to any attribution statement specified by the Information Provider(s) and, where possible, provide a link to this licence;\n\nIf the Information Provider does not provide a specific attribution statement, you must use the following:\n\nContains public sector information licensed under the Open Government Licence v3.0.\n\nIf you are using Information from several Information Providers and listing multiple attributions is not practical in your product or application, you may include a URI or hyperlink to a resource that contains the required attribution statements.\nThese are important conditions of this licence and if you fail to comply with them the rights granted to you under this licence, or any similar licence granted by the Licensor, will end automatically.\n\n\n\nThis licence does not cover:\n\npersonal data in the Information;\nInformation that has not been accessed by way of publication or disclosure under information access legislation (including the Freedom of Information Acts for the UK and Scotland) by or with the consent of the Information Provider;\ndepartmental or public sector organisation logos, crests and the Royal Arms except where they form an integral part of a document or dataset;\nmilitary insignia;\nthird party rights the Information Provider is not authorised to license;\nother intellectual property rights, including patents, trade marks, and design rights; and\nidentity documents such as the British Passport\n\n\n\n\nThis licence does not grant you any right to use the Information in a way that suggests any official status or that the Information Provider and/or Licensor endorse you or your use of the Information.\n\n\n\nThe Information is licensed ‘as is’ and the Information Provider and/or Licensor excludes all representations, warranties, obligations and liabilities in relation to the Information to the maximum extent permitted by law.\nThe Information Provider and/or Licensor are not liable for any errors or omissions in the Information and shall not be liable for any loss, injury or damage of any kind caused by its use. The Information Provider does not guarantee the continued supply of the Information.\n\n\n\nThis licence is governed by the laws of the jurisdiction in which the Information Provider has its principal place of business, unless otherwise specified by the Information Provider.\n\n\n\nIn this licence, the terms below have the following meanings:\n‘Information’ means information protected by copyright or by database right (for example, literary and artistic works, content, data and source code) offered for use under the terms of this licence.\n‘Information Provider’ means the person or organisation providing the Information under this licence.\n‘Licensor’ means any Information Provider which has the authority to offer Information under the terms of this licence or the Keeper of Public Records, who has the authority to offer Information subject to Crown copyright and Crown database rights and Information subject to copyright and database right that has been assigned to or acquired by the Crown, under the terms of this licence.\n‘Use’ means doing any act which is restricted by copyright or database right, whether in the original medium or in any other medium, and includes without limitation distributing, copying, adapting, modifying as may be technically necessary to use it in a different mode or format.\n‘You’, ‘you’ and ‘your’ means the natural or legal person, or body of persons corporate or incorporate, acquiring rights in the Information (whether the Information is obtained directly from the Licensor or otherwise) under this licence.\n\n\n\nThe National Archives has developed this licence as a tool to enable Information Providers in the public sector to license the use and re-use of their Information under a common open licence. The National Archives invites public sector bodies owning their own copyright and database rights to permit the use of their Information under this licence.\nThe Keeper of the Public Records has authority to license Information subject to copyright and database right owned by the Crown. The extent of the offer to license this Information under the terms of this licence is set out in the UK Government Licensing Framework. http://www.nationalarchives.gov.uk/information-management/re-using-public-sector-information/re-use-and-licensing/ukglf/\nThis is version 3.0 of the Open Government Licence. The National Archives may, from time to time, issue new versions of the Open Government Licence. If you are already using Information under a previous version of the Open Government Licence, the terms of that licence will continue to apply.\nThese terms are compatible with the Creative Commons Attribution License 4.0 and the Open Data Commons Attribution License, both of which license copyright and database rights. This means that when the Information is adapted and licensed under either of those licences, you automatically satisfy the conditions of the OGL when you comply with the other licence. The OGLv3.0 is Open Definition compliant.\nFurther context, best practice and guidance can be found in the UK Government Licensing Framework section on The National Archives website. http://www.nationalarchives.gov.uk/information-management/re-using-public-sector-information/uk-government-licensing-framework/\nOpen Government License for public sector information"
  },
  {
    "objectID": "licence.html#using-information-under-this-licence",
    "href": "licence.html#using-information-under-this-licence",
    "title": "1 Open Government Licence v3",
    "section": "",
    "text": "Use of copyright and database right material expressly made available under this licence (the ‘Information’) indicates your acceptance of the terms and conditions below.\nThe Licensor grants you a worldwide, royalty-free, perpetual, non-exclusive licence to use the Information subject to the conditions below.\nThis licence does not affect your freedom under fair dealing or fair use or any other copyright or database right exceptions and limitations.\nYou are free to:\n\ncopy, publish, distribute and transmit the Information;\nadapt the Information;\nexploit the Information commercially and non-commercially for example, by combining it with other Information, or by including it in your own product or application.\n\nYou must (where you do any of the above):\n\nacknowledge the source of the Information in your product or application by including or linking to any attribution statement specified by the Information Provider(s) and, where possible, provide a link to this licence;\n\nIf the Information Provider does not provide a specific attribution statement, you must use the following:\n\nContains public sector information licensed under the Open Government Licence v3.0.\n\nIf you are using Information from several Information Providers and listing multiple attributions is not practical in your product or application, you may include a URI or hyperlink to a resource that contains the required attribution statements.\nThese are important conditions of this licence and if you fail to comply with them the rights granted to you under this licence, or any similar licence granted by the Licensor, will end automatically."
  },
  {
    "objectID": "licence.html#exemptions",
    "href": "licence.html#exemptions",
    "title": "1 Open Government Licence v3",
    "section": "",
    "text": "This licence does not cover:\n\npersonal data in the Information;\nInformation that has not been accessed by way of publication or disclosure under information access legislation (including the Freedom of Information Acts for the UK and Scotland) by or with the consent of the Information Provider;\ndepartmental or public sector organisation logos, crests and the Royal Arms except where they form an integral part of a document or dataset;\nmilitary insignia;\nthird party rights the Information Provider is not authorised to license;\nother intellectual property rights, including patents, trade marks, and design rights; and\nidentity documents such as the British Passport"
  },
  {
    "objectID": "licence.html#non-endorsement",
    "href": "licence.html#non-endorsement",
    "title": "1 Open Government Licence v3",
    "section": "",
    "text": "This licence does not grant you any right to use the Information in a way that suggests any official status or that the Information Provider and/or Licensor endorse you or your use of the Information."
  },
  {
    "objectID": "licence.html#no-warranty",
    "href": "licence.html#no-warranty",
    "title": "1 Open Government Licence v3",
    "section": "",
    "text": "The Information is licensed ‘as is’ and the Information Provider and/or Licensor excludes all representations, warranties, obligations and liabilities in relation to the Information to the maximum extent permitted by law.\nThe Information Provider and/or Licensor are not liable for any errors or omissions in the Information and shall not be liable for any loss, injury or damage of any kind caused by its use. The Information Provider does not guarantee the continued supply of the Information."
  },
  {
    "objectID": "licence.html#governing-law",
    "href": "licence.html#governing-law",
    "title": "1 Open Government Licence v3",
    "section": "",
    "text": "This licence is governed by the laws of the jurisdiction in which the Information Provider has its principal place of business, unless otherwise specified by the Information Provider."
  },
  {
    "objectID": "licence.html#definitions",
    "href": "licence.html#definitions",
    "title": "1 Open Government Licence v3",
    "section": "",
    "text": "In this licence, the terms below have the following meanings:\n‘Information’ means information protected by copyright or by database right (for example, literary and artistic works, content, data and source code) offered for use under the terms of this licence.\n‘Information Provider’ means the person or organisation providing the Information under this licence.\n‘Licensor’ means any Information Provider which has the authority to offer Information under the terms of this licence or the Keeper of Public Records, who has the authority to offer Information subject to Crown copyright and Crown database rights and Information subject to copyright and database right that has been assigned to or acquired by the Crown, under the terms of this licence.\n‘Use’ means doing any act which is restricted by copyright or database right, whether in the original medium or in any other medium, and includes without limitation distributing, copying, adapting, modifying as may be technically necessary to use it in a different mode or format.\n‘You’, ‘you’ and ‘your’ means the natural or legal person, or body of persons corporate or incorporate, acquiring rights in the Information (whether the Information is obtained directly from the Licensor or otherwise) under this licence."
  },
  {
    "objectID": "licence.html#about-the-open-government-licence",
    "href": "licence.html#about-the-open-government-licence",
    "title": "1 Open Government Licence v3",
    "section": "",
    "text": "The National Archives has developed this licence as a tool to enable Information Providers in the public sector to license the use and re-use of their Information under a common open licence. The National Archives invites public sector bodies owning their own copyright and database rights to permit the use of their Information under this licence.\nThe Keeper of the Public Records has authority to license Information subject to copyright and database right owned by the Crown. The extent of the offer to license this Information under the terms of this licence is set out in the UK Government Licensing Framework. http://www.nationalarchives.gov.uk/information-management/re-using-public-sector-information/re-use-and-licensing/ukglf/\nThis is version 3.0 of the Open Government Licence. The National Archives may, from time to time, issue new versions of the Open Government Licence. If you are already using Information under a previous version of the Open Government Licence, the terms of that licence will continue to apply.\nThese terms are compatible with the Creative Commons Attribution License 4.0 and the Open Data Commons Attribution License, both of which license copyright and database rights. This means that when the Information is adapted and licensed under either of those licences, you automatically satisfy the conditions of the OGL when you comply with the other licence. The OGLv3.0 is Open Definition compliant.\nFurther context, best practice and guidance can be found in the UK Government Licensing Framework section on The National Archives website. http://www.nationalarchives.gov.uk/information-management/re-using-public-sector-information/uk-government-licensing-framework/\nOpen Government License for public sector information"
  },
  {
    "objectID": "images/images_readme.html",
    "href": "images/images_readme.html",
    "title": "1 Python Jupyter Repo Template",
    "section": "",
    "text": "1 Python Jupyter Repo Template\n\n1.0.1 Version 1.0\nCourse Summary\nA template to use when creating a new repository for Python content, using Jupiter Notebooks for delivery.\nLearning Outcome\nNA\nLead Developer\nRichard Leyshon\nCourse Reviewer(s)\nPending\nIntended Audience\nFaculty\nLearning Objective\nAt the end of the course, participants will be able to:-\n\nNA\n\nCourse Type\n\nE learning - Not Available\nSelf learning - Not Available\nFace to face - Not Available\n\nSkill Level\nNA\nPre requisite summary\nNA"
  },
  {
    "objectID": "course_content/Chapter 3 - Generalised Linear Models.html",
    "href": "course_content/Chapter 3 - Generalised Linear Models.html",
    "title": "Chapter 3 -Generalised Linear Models",
    "section": "",
    "text": "The traditional statistical approach was to assume that all variation in the data was normally distributed, or to transform the data until it was, and then use classical methods based on the normal distribution to draw conclusions.\nIn generalized linear models, variability isn’t just a nuisance, but actually tells us something about the processes we are interested in. What we treat as “signal” and what we treat as “noise” depends on our question. The same source of variability might provide an interesting insight into the data or be something we wish to account for so we can explore it further.\nIn the next section, we will introduce you to a few common distributions that capture different forms of variation in the response variable from the exponential family.\nWe will then give practical examples of how to fit and interpret these models using python.\n\n\nWhy is probability important?\nUncertainty and variability occur in many aspects of life. Probability helps us make sense and quantify uncertainties. Probability also helps us make informed decisions on what is likely to happened, based on patterns in the data collected.\n\nProbability is the chance of an event occurring, defined between 0 and 1.\n\n0 = will never happen, 1 = will always happen\n\nRandom Variable is the variable that takes values depending on outcomes of random events. A random variable can be either discrete (having specific values, e.g. country) or continuous (any value in a continuous range, e.g. age, weight).\nProbability distribution the probability distribution for a random variable describes how the probabilities are distributed over the values of the random variable.\n\nFor a discrete random variable, x, the probability distribution is defined by a probability mass function, denoted by \\(f(x)\\).\nFor a continuous random variable, because there is an infinite number of values in any interval (e.g. you can always go to smaller and smaller decimals), we instead consider the probability that a continuous random variable will lie within a given interval. Here the probability distribution is defined by a probability density function, also denoted by \\(f(x)\\).\n\n\nBoth probability functions must satisfy two requirements:\n\n\\(f(x)\\) must be non-negative for each value of the random variable.\nThe sum of the probabilities for each value (or integral over all values) of the random variable must equal one.\n\n\n\n\nNow that we know how to define a probability for discrete and continuous random variables we can take a look at a few common probability distributions from the exponential family that we can use for our response variable including Normal, Binomial, Poisson, and Negative Binomial. There is a large class of generalized linear models that we can use to model these different distributions.\n\n\nWe’ve already come across the normal distribution, which we use to inspect and compare our residuals. The normal distribution is arguably the most commonly used distribution in statistics. It is used for continuous variables (e.g. height, measurement error). It has several computational properties which make it easy to work with. For instance, it is symmetric, unimodal, and the mean, median, and mode are all equal. That means it can be captured just by specifying the mean and the variance.\nYou may use it to represent changes in population. Let’s say you know the population mean but wish to understand how much it varies (e.g. by +/- 10%, +/- 15%).\nBelow we have two plots of the normal distribution, where the mean is the same: 0, but with higher (5) or lower (1) variance.\n\n# Import required packages\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\nimport matplotlib.pyplot as plt # data plotting\nimport seaborn as sns # data visualisation and plotting\nimport statsmodels.api as sm # statistical modelling package\nimport statsmodels.formula.api as smf # statistical modelling package with R-like formulas\nimport scipy.stats as stats # statistical distributions\nimport math # mathetical operations\nfrom statsmodels.genmod.generalized_linear_model import GLM # importing packages to run GLM\nfrom statsmodels.genmod import families # importing families for exponential families\nfrom pandas.plotting import register_matplotlib_converters # additional package support for plotting with matplotlib\nfrom scipy.stats import poisson # poisson distribution\nfrom scipy.stats import binom # binomial distribution\nfrom scipy.stats import nbinom # negative binomial distribution\nfrom scipy.stats import norm # normal distribution\nfrom scipy.stats import chi2 # Chi squared statistic\nfrom IPython.display import display # Public API for display tools in IPython.\nimport warnings # warning functionality\n\n# Seaborn plot default style change\nplt.style.use('ggplot')\n\n# supress warnings due to different versions of packages\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n\n## Normal Distribution\n\n# A normal distribution with mean 0 and variance 1\nmean = 0 # mean\nvariance = 1 # variance\nstddev = math.sqrt(variance) # standard deviation\n# random generated sample of \n# the normal distribution to plot\nnorm_1 = np.linspace(mean - 3*stddev, mean + 3*stddev, 100) # creates the range of x values\n\n# Add the normal distribution to plot object\nplt.plot(norm_1, stats.norm.pdf(norm_1, mean, stddev))\n\n# A normal distribution with mean 10 and variance 10\nmean = 10 # mean\nvariance = 10 # variance\nstddev = math.sqrt(variance) # std deviation\n\n# random generated sample of \n# the normal distribution to plot\nnorm_2 = np.linspace(mean - 3*stddev, mean + 3*stddev, 100) # creates the range of x values\n\n# Add the second normal distribution to plot object, label and show\nplt.plot(norm_2, stats.norm.pdf(norm_2, mean, stddev))\nplt.ylabel('Density')\nplt.xlabel('Value')\nplt.show()\n\n\n\n\n\n\n\n\nThe normal distribution is useful for modelling continuous data. In the earlier example, it was appropriate to assume that our response variable (salary) was normally distributed given that it is a measured continuous variable.\n\n\n\nThe binomial distribution applies when you have a series of trials where the outcome of the trial can only take one of two values (e.g. heads/tails, alive/dead, present/absent, or buyers/non-buyers).\nThe binomial distribution can be written as:\n\\(P(\\text{k out of n}) = \\frac{n!}{k!(n-k)!}p^k(1-p)^{n-k}\\)\nwhere \\(n\\) is the number of trials, \\(k\\) is the number of successes, and \\(p\\) is the probability of success.\nWe can then get the mean as \\(\\mu = np\\) and the variance is \\(\\sigma^2 = np(1-p)\\).\nIn the case where there is only one trial (e.g. one coin toss) we can use the bernoulli distribution. The bernoulli distribution is a special case of the Binomial distribution, and is used to describe an instance of the binomial distribution where there is only a single trial. So if we have one coin toss, it can only have two outcomes: heads (0) or tails (1).\n\n# Generate the possible values for the number of successes, k, \n# in this case, heads (0), tails (1). \nk = np.arange(2) \n\n# If the coin is fair, we have a 50/50 chance of \n# getting tails (1), i.e. 50% probability.\ndata_binom = binom.pmf(k, n=1, p=0.5) \n\n# Plot the results\nax_binom = sns.barplot(x=k, y=data_binom)\nax_binom.set(xlabel='Tails', ylabel='Density')\nax_binom;\n\n\n\n\n\n\n\n\nIf we have two coin tosses, we have 4 possible combinations: heads/heads, tails/heads, heads/tails, tails/tails. Which result in 3 outcomes: heads/heads (0+0=0), tails/tails (1+1 = 2) and heads/tails (0+1 = 1) (because the order doesn’t matter).\n\n# Generate the number of successes, k, \n# in this case, heads/heads (0), tails/heads (1), tails/tails (2).\nk = np.arange(3)  \n\n# If the coin is fair, \n# we have a 50/50 chance of heads or tails, i.e. 50% probability.\ndata_binom = binom.pmf(k, n=2, p=0.5) \n\n# Plot the results\nax_binom = sns.barplot(x = k, y = data_binom)\nax_binom.set(xlabel='Tails', ylabel='Density')\nax_binom;\n\n\n\n\n\n\n\n\nI may wish to know how many students in my course like statistics before I begin. I can start by asking: * What is the probability that any random student likes statistics? * 0.7? 0.8?\nI’m feeling pretty positive, there’s an 80% chance that a random student likes statistics. But what is the probability that the whole class likes statistics? I could ask: * What is the probability that all 10 students in the course like statistics? * 0.8 x 0.8 x 0.8 … x 0.8 = \\(0.8^{10} \\approx 0.107\\) :-(\nWe can get to this answer by multiplying the probability of each student liking statistics, in this case \\(0.8^{10}\\) which is equal to \\(\\approx 0.107\\) or 10.7% chance of all students in a class of 10 liking statistics.\n10.7% doesn’t sound very promising. But I would settle for 6-9 people out of 10 liking statistics. Of course, I won’t know the answer until I attend the course, but I can get an idea of how probable it is that 6/10 or 7/10 of you like probability by working out the probability mass function:\n\n# Generate the number of successes, k, in this case, \n# students from 0 to 10 who like the course. \nk = np.arange(11)\n\n# We can compute the probability of 0/10, 1/10... 10/10 students liking the course. \n# Where n is the number of trials (in this case the number of students).\ndata_binom = binom.pmf(k=k, n=10, p=0.8) \n\n# Plot the results\nax_binom = sns.barplot(x=k, y=data_binom)\nax_binom.set(xlabel='Successes', ylabel='Density')\nax_binom;\n\n\n\n\n\n\n\n\n\n# this is what our binomial data created looks like\ndata_binom\n\narray([1.02400000e-07, 4.09600000e-06, 7.37280000e-05, 7.86432000e-04,\n       5.50502400e-03, 2.64241152e-02, 8.80803840e-02, 2.01326592e-01,\n       3.01989888e-01, 2.68435456e-01, 1.07374182e-01])\n\n\nThe binom distribution function from scipy.stats takes n and p as shape arguments, where n is the number of trials, and p is the probability,\nIn the code above we have specified that we are asking 10 random students in each trial (n), that the probability that a student likes statistics is 0.8. From the probability mass function density plot it looks quite likely that between 6-9 students will like statistics. Hooray!\nSome examples of where we might use the binomial are to model\n\nThe number of individuals with malaria out of the number of individuals in the population (e.g. an individual can have malaria (1) or not (0))\nThe number of customers who bought item A out of total number of shop customers (a customer can have bought item A (1) or not (0))\n\nWhere the number of outcomes is greater than 2, e.g. (single, married, divorced), then we can use a multinomial distribution which allows for more than 2 possibilities. We can get our clue for the number of outcomes from the name (bi = 2, multi = multiple).\n\n\n\nThe Poisson distribution gives the distribution of the number of individuals, arrivals, events, counts, etc., in a given time/space/unit of counting effort.\nThis is the common distribution to use whenever things are counted and when they are discrete (e.g. you cannot have 1.5 of a person).\n\nImage Credit: Bjørn Christian Tørrissen CC BY-SA 3.0 via Wikimedia Commons.\nThe Poisson distribution expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known constant rate and independently of the time since the last event. For instance, we could use the Poisson distribution to model the number of cyclists crossing a bridge per day. However, the Poisson distribution can also be used for the number of events in other specified intervals such as distance, area or volume.\nThe Poisson distribution is given by the formula:\n\\[P(k\\ \\mathrm{events}) = \\frac{\\lambda^{k}e^{-\\lambda}}{k!} \\]\nWhere, \\(\\lambda\\) is the average density of the events (e.g. cyclists per day), and the function returns the likelihood of seeing \\(k\\) cyclists.\nIn the Poisson distribution, the mean and the variance are the same, which is why the poisson function takes only the mean (mu or \\(\\mu\\)) in the code below. \\(\\mu\\) is also referred to as the shape parameter in the distribution.\n\n# generate possible x values for plot representing k events.\nk = np.arange(250, step = 10)\n\n# create poisson distribution for range of x\ndata_poisson = poisson.pmf(k=k, mu=150)\n\n# plot the distribution\nax_poisson = sns.barplot(x=k, y=data_poisson)\nax_poisson.set(xlabel='Count', ylabel='Density')\nplt.xticks(rotation=90);\n\n\n\n\n\n\n\n\nAs mentioned before the poisson is the most common distribution we use to model counts (events, people, animals, etc.). However, we are constrained by the fact that the mean and variance are the same. This means that more counts, and as a result larger means allow for greater variation, whereas fewer counts with a smaller means allow for less variation. If we need to allow for more variation (i.e. wider distribution of counts) we can look to the negative binomial.\n\nFun Fact: One of the most famous historical, practical uses of the Poisson distribution was estimating the annual number of Prussian cavalry soldiers killed due to horse-kicks.\n\n\n\n\nThe negative binomial distribution is discrete, like the Poisson, but its variance can be larger than its mean, (i.e. it can be overdispersed). It is often used when the data largely follows a Poisson distribution but there are a large number of zeros in the data (e.g. few sightings and a large number of absences).\nIn fact, the negative binomial distribution is a generalisation of the poisson distribution, whose variance is considered separately from its mean, and hence works well when data are overdispersed. The negative binomial is usually characterised in similar terms to the binomial distribution - \\(r\\) (number of failures observed) and \\(p\\) (probability of success), with \\(k\\) now representing the probability of seeing \\(k\\) successes before \\(r\\) failures are observed given \\(p\\).\nThis is how it is done in scipy.stats, however it can also be parameterised in terms of its mean and variance. As a result, the mean and variance do not have to be equal as with the Poisson distribution, which makes it more flexible.\n\\[p = \\frac{\\sigma^{2} - \\mu}{\\sigma^{2}}\\]\n\\[r = \\frac{\\mu^{2}}{\\sigma^{2} - \\mu}\\]\nGiving the Negative Binomial distribution as:\n\\[P(X = k) = \\left(\n    \\begin{matrix}\n        k + r - 1\\\\\n        k\n    \\end{matrix}\n\\right)p^{k} \\left( \\frac{\\mu}{\\sigma^{2}}  \\right)^{r} \\]\nWhere:\n\\(p\\) is the probability of success in a Bernoulli trial with two outcomes (success or failure);\n\\(r\\) is the dispersion or shape parameter;\n\\(k\\) is the given number of successes of interest.\nNote that although commonly called \\(r\\), scipy refers to this parameter as \\(n\\).\nIn scipy.stats, nbinom takes n and p as shape parameters where n is the number of successes, whereas p is the probability of a single success.\nIn the code below we are showing a different approach, in this case we are generating random values from the negative binomial distribution. We are plotting on the x axis the number of successes (e.g. these could be counts of animals or numbers of people) before we get our first failure (e.g. no animal or no person detected).\nIn this case size is the number of times we run the trial. n is the number of successes before a failure, and p is the probability of a success.\n\n# generate the data from the negative binomial distribution\ndata_nbinom = nbinom.rvs(n=5, p=0.5, size=1000)\n\n# plot the distribution\nax_nbinom = sns.distplot(data_nbinom, kde=False)\n\n# add labels\nax_nbinom.set(xlabel='Negative Binomial', ylabel='Frequency')\nax_nbinom;\n\nC:\\Users\\tbalb\\AppData\\Local\\Temp\\ipykernel_1884\\2654644825.py:5: UserWarning:\n\n\n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n\n\n\n\n\n\n\n\n\n\nFrom this we can see that 4 successes before we see a failure is the most frequent value, followed by 5 successes. You’ll notice that we also get a frequency for 0 to 30 successes. We can transform these into probabilities by dividing the frequency by the size (number of trials) we ran.\nThe negative binomial, may perhaps seem a little abstract to relate the number of successes before a failure to counts, but the important information to take away is that the negative binomial can be an effective way to model count data that is overdispersed (i.e. the data has a larger variance than the mean).\n\n\n\n\n\nQuestion 1Answer\n\n\n\nReflect on some examples from your own work. Write a short paragraph or discuss within a group of where you have a response variable that comes from a Binomial, Poisson, Negative binomial or Normal distribution?\nExperiment with the poisson distribution. What happens when you experiment with the shape parameter, mu?\nExperiment with the normal distribution. What happens when you change the variance? What happens when you change the mean?\n\n\n\n\nYour examples will vary depend on your area of work. Response variables that come from a binomial distribution might include a model where you are looking at two outcomes e.g. presence/absence, dead/alive etc. A Poisson response variable will typically be a count, e.g. how many bicycles crossed the Brooklyn Bridge, numbers of wildebeest, number of times a mobile phone game has been played in a day. These are discrete counts, you can’t have 4.5 of a wildebeest or 5.5 bicycles crossing the Brookly bridge. Response variables with a negative binomial distribution are a bit harder to detect, but they might include outcomes where you have a lot of zeros, e.g. if you typically only see pods of whales and have a lot of instances with zero whales observed, or where the data is more variable some very high counts and some low counts. Normally distributed response variable might include measurements such as weight or height. It is typically used for continuous variables.\nExperiment with the poisson distribution. What happens when you experiment with the shape parameter, mu? We observe that the Poisson distributions are unimodal (one peak); exhibit positive skew (that decreases as mu increases); are centred roughly on mu; have variance (spread) that increases as mu increases.\nExperiment with the normal distribution. What happens when you change the variance? What happens when you change the mean? Changing the mean will shift where the normal distribution is centered. Increasing the variance will widen the distribution (more flat), decreasing the variance will tighten the distribution (more peaked).\n\n\n\n\n\n\n\nA generalized linear model consists of 3 parts:\n\nAn exponential family probability distribution\nA linear predictor\nA link function\n\nThe exponential family probability distribution, is the probability distribution that our response variable follows, for instance, Normal, Binomial, Poisson, and Negative Binomial.\nThe linear predictor is a linear combination of a set of coefficients and explanatory variables used to predict our response variable. This is essentially the same form used to specify our linear model, which means we can use the same formula set up for our GLM.\nThe link function allows us to use the linear predictor by providing the link to connect the exponential distribution to the linear predictor. There will often be defaults for the link when you call the model family, but you can also specify this yourself. For instance, the Poisson distribution is a discrete distribution where values can take on integers between 0 and infinity. However, the linear predictor is based on the normal distribution that is continuous and can go from -infinity to infinity. It is therefore common to use a log link to log the response variable, which effectively limits the normal distribution to 0 to infinity, thereby making it suitable for the Poisson distribution. This means that we often need to do a back transformation when we interpret the parameter coefficients in the actual scale. For example, when using the log-link we would exponentiate the coefficients.\nSpecifying the exponential family\nTo specify the exponential family we add an additional argument in our model specification, family = sm.families.Binomial(). There are several families we can specify, here are the main ones we have covered so far:\n\nsm.families.Binomial()\nsm.families.Gaussian() *i.e. normal distribution\nsm.families.Poisson()\nsm.families.NegativeBinomial()\n\nYou change the link from the default, according to the model you wish to fit, by specifying it as follows:\n\nsm.families.Binomial(link='probit')\n\nWe then specify the formula the same as before (y ~ x).\nformula = 'y ~ x'\nmod1 = smf.GLM(formula = formula, data = df, family = sm.families.Binomial()).fit()\nmod1.summary()",
    "crumbs": [
      "3 - Generalised Linear Models"
    ]
  },
  {
    "objectID": "course_content/Chapter 3 - Generalised Linear Models.html#probability-and-random-variables-discrete-and-continuous",
    "href": "course_content/Chapter 3 - Generalised Linear Models.html#probability-and-random-variables-discrete-and-continuous",
    "title": "Chapter 3 -Generalised Linear Models",
    "section": "",
    "text": "Why is probability important?\nUncertainty and variability occur in many aspects of life. Probability helps us make sense and quantify uncertainties. Probability also helps us make informed decisions on what is likely to happened, based on patterns in the data collected.\n\nProbability is the chance of an event occurring, defined between 0 and 1.\n\n0 = will never happen, 1 = will always happen\n\nRandom Variable is the variable that takes values depending on outcomes of random events. A random variable can be either discrete (having specific values, e.g. country) or continuous (any value in a continuous range, e.g. age, weight).\nProbability distribution the probability distribution for a random variable describes how the probabilities are distributed over the values of the random variable.\n\nFor a discrete random variable, x, the probability distribution is defined by a probability mass function, denoted by \\(f(x)\\).\nFor a continuous random variable, because there is an infinite number of values in any interval (e.g. you can always go to smaller and smaller decimals), we instead consider the probability that a continuous random variable will lie within a given interval. Here the probability distribution is defined by a probability density function, also denoted by \\(f(x)\\).\n\n\nBoth probability functions must satisfy two requirements:\n\n\\(f(x)\\) must be non-negative for each value of the random variable.\nThe sum of the probabilities for each value (or integral over all values) of the random variable must equal one.",
    "crumbs": [
      "3 - Generalised Linear Models"
    ]
  },
  {
    "objectID": "course_content/Chapter 3 - Generalised Linear Models.html#the-exponential-family",
    "href": "course_content/Chapter 3 - Generalised Linear Models.html#the-exponential-family",
    "title": "Chapter 3 -Generalised Linear Models",
    "section": "",
    "text": "Now that we know how to define a probability for discrete and continuous random variables we can take a look at a few common probability distributions from the exponential family that we can use for our response variable including Normal, Binomial, Poisson, and Negative Binomial. There is a large class of generalized linear models that we can use to model these different distributions.\n\n\nWe’ve already come across the normal distribution, which we use to inspect and compare our residuals. The normal distribution is arguably the most commonly used distribution in statistics. It is used for continuous variables (e.g. height, measurement error). It has several computational properties which make it easy to work with. For instance, it is symmetric, unimodal, and the mean, median, and mode are all equal. That means it can be captured just by specifying the mean and the variance.\nYou may use it to represent changes in population. Let’s say you know the population mean but wish to understand how much it varies (e.g. by +/- 10%, +/- 15%).\nBelow we have two plots of the normal distribution, where the mean is the same: 0, but with higher (5) or lower (1) variance.\n\n# Import required packages\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\nimport matplotlib.pyplot as plt # data plotting\nimport seaborn as sns # data visualisation and plotting\nimport statsmodels.api as sm # statistical modelling package\nimport statsmodels.formula.api as smf # statistical modelling package with R-like formulas\nimport scipy.stats as stats # statistical distributions\nimport math # mathetical operations\nfrom statsmodels.genmod.generalized_linear_model import GLM # importing packages to run GLM\nfrom statsmodels.genmod import families # importing families for exponential families\nfrom pandas.plotting import register_matplotlib_converters # additional package support for plotting with matplotlib\nfrom scipy.stats import poisson # poisson distribution\nfrom scipy.stats import binom # binomial distribution\nfrom scipy.stats import nbinom # negative binomial distribution\nfrom scipy.stats import norm # normal distribution\nfrom scipy.stats import chi2 # Chi squared statistic\nfrom IPython.display import display # Public API for display tools in IPython.\nimport warnings # warning functionality\n\n# Seaborn plot default style change\nplt.style.use('ggplot')\n\n# supress warnings due to different versions of packages\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n\n## Normal Distribution\n\n# A normal distribution with mean 0 and variance 1\nmean = 0 # mean\nvariance = 1 # variance\nstddev = math.sqrt(variance) # standard deviation\n# random generated sample of \n# the normal distribution to plot\nnorm_1 = np.linspace(mean - 3*stddev, mean + 3*stddev, 100) # creates the range of x values\n\n# Add the normal distribution to plot object\nplt.plot(norm_1, stats.norm.pdf(norm_1, mean, stddev))\n\n# A normal distribution with mean 10 and variance 10\nmean = 10 # mean\nvariance = 10 # variance\nstddev = math.sqrt(variance) # std deviation\n\n# random generated sample of \n# the normal distribution to plot\nnorm_2 = np.linspace(mean - 3*stddev, mean + 3*stddev, 100) # creates the range of x values\n\n# Add the second normal distribution to plot object, label and show\nplt.plot(norm_2, stats.norm.pdf(norm_2, mean, stddev))\nplt.ylabel('Density')\nplt.xlabel('Value')\nplt.show()\n\n\n\n\n\n\n\n\nThe normal distribution is useful for modelling continuous data. In the earlier example, it was appropriate to assume that our response variable (salary) was normally distributed given that it is a measured continuous variable.\n\n\n\nThe binomial distribution applies when you have a series of trials where the outcome of the trial can only take one of two values (e.g. heads/tails, alive/dead, present/absent, or buyers/non-buyers).\nThe binomial distribution can be written as:\n\\(P(\\text{k out of n}) = \\frac{n!}{k!(n-k)!}p^k(1-p)^{n-k}\\)\nwhere \\(n\\) is the number of trials, \\(k\\) is the number of successes, and \\(p\\) is the probability of success.\nWe can then get the mean as \\(\\mu = np\\) and the variance is \\(\\sigma^2 = np(1-p)\\).\nIn the case where there is only one trial (e.g. one coin toss) we can use the bernoulli distribution. The bernoulli distribution is a special case of the Binomial distribution, and is used to describe an instance of the binomial distribution where there is only a single trial. So if we have one coin toss, it can only have two outcomes: heads (0) or tails (1).\n\n# Generate the possible values for the number of successes, k, \n# in this case, heads (0), tails (1). \nk = np.arange(2) \n\n# If the coin is fair, we have a 50/50 chance of \n# getting tails (1), i.e. 50% probability.\ndata_binom = binom.pmf(k, n=1, p=0.5) \n\n# Plot the results\nax_binom = sns.barplot(x=k, y=data_binom)\nax_binom.set(xlabel='Tails', ylabel='Density')\nax_binom;\n\n\n\n\n\n\n\n\nIf we have two coin tosses, we have 4 possible combinations: heads/heads, tails/heads, heads/tails, tails/tails. Which result in 3 outcomes: heads/heads (0+0=0), tails/tails (1+1 = 2) and heads/tails (0+1 = 1) (because the order doesn’t matter).\n\n# Generate the number of successes, k, \n# in this case, heads/heads (0), tails/heads (1), tails/tails (2).\nk = np.arange(3)  \n\n# If the coin is fair, \n# we have a 50/50 chance of heads or tails, i.e. 50% probability.\ndata_binom = binom.pmf(k, n=2, p=0.5) \n\n# Plot the results\nax_binom = sns.barplot(x = k, y = data_binom)\nax_binom.set(xlabel='Tails', ylabel='Density')\nax_binom;\n\n\n\n\n\n\n\n\nI may wish to know how many students in my course like statistics before I begin. I can start by asking: * What is the probability that any random student likes statistics? * 0.7? 0.8?\nI’m feeling pretty positive, there’s an 80% chance that a random student likes statistics. But what is the probability that the whole class likes statistics? I could ask: * What is the probability that all 10 students in the course like statistics? * 0.8 x 0.8 x 0.8 … x 0.8 = \\(0.8^{10} \\approx 0.107\\) :-(\nWe can get to this answer by multiplying the probability of each student liking statistics, in this case \\(0.8^{10}\\) which is equal to \\(\\approx 0.107\\) or 10.7% chance of all students in a class of 10 liking statistics.\n10.7% doesn’t sound very promising. But I would settle for 6-9 people out of 10 liking statistics. Of course, I won’t know the answer until I attend the course, but I can get an idea of how probable it is that 6/10 or 7/10 of you like probability by working out the probability mass function:\n\n# Generate the number of successes, k, in this case, \n# students from 0 to 10 who like the course. \nk = np.arange(11)\n\n# We can compute the probability of 0/10, 1/10... 10/10 students liking the course. \n# Where n is the number of trials (in this case the number of students).\ndata_binom = binom.pmf(k=k, n=10, p=0.8) \n\n# Plot the results\nax_binom = sns.barplot(x=k, y=data_binom)\nax_binom.set(xlabel='Successes', ylabel='Density')\nax_binom;\n\n\n\n\n\n\n\n\n\n# this is what our binomial data created looks like\ndata_binom\n\narray([1.02400000e-07, 4.09600000e-06, 7.37280000e-05, 7.86432000e-04,\n       5.50502400e-03, 2.64241152e-02, 8.80803840e-02, 2.01326592e-01,\n       3.01989888e-01, 2.68435456e-01, 1.07374182e-01])\n\n\nThe binom distribution function from scipy.stats takes n and p as shape arguments, where n is the number of trials, and p is the probability,\nIn the code above we have specified that we are asking 10 random students in each trial (n), that the probability that a student likes statistics is 0.8. From the probability mass function density plot it looks quite likely that between 6-9 students will like statistics. Hooray!\nSome examples of where we might use the binomial are to model\n\nThe number of individuals with malaria out of the number of individuals in the population (e.g. an individual can have malaria (1) or not (0))\nThe number of customers who bought item A out of total number of shop customers (a customer can have bought item A (1) or not (0))\n\nWhere the number of outcomes is greater than 2, e.g. (single, married, divorced), then we can use a multinomial distribution which allows for more than 2 possibilities. We can get our clue for the number of outcomes from the name (bi = 2, multi = multiple).\n\n\n\nThe Poisson distribution gives the distribution of the number of individuals, arrivals, events, counts, etc., in a given time/space/unit of counting effort.\nThis is the common distribution to use whenever things are counted and when they are discrete (e.g. you cannot have 1.5 of a person).\n\nImage Credit: Bjørn Christian Tørrissen CC BY-SA 3.0 via Wikimedia Commons.\nThe Poisson distribution expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known constant rate and independently of the time since the last event. For instance, we could use the Poisson distribution to model the number of cyclists crossing a bridge per day. However, the Poisson distribution can also be used for the number of events in other specified intervals such as distance, area or volume.\nThe Poisson distribution is given by the formula:\n\\[P(k\\ \\mathrm{events}) = \\frac{\\lambda^{k}e^{-\\lambda}}{k!} \\]\nWhere, \\(\\lambda\\) is the average density of the events (e.g. cyclists per day), and the function returns the likelihood of seeing \\(k\\) cyclists.\nIn the Poisson distribution, the mean and the variance are the same, which is why the poisson function takes only the mean (mu or \\(\\mu\\)) in the code below. \\(\\mu\\) is also referred to as the shape parameter in the distribution.\n\n# generate possible x values for plot representing k events.\nk = np.arange(250, step = 10)\n\n# create poisson distribution for range of x\ndata_poisson = poisson.pmf(k=k, mu=150)\n\n# plot the distribution\nax_poisson = sns.barplot(x=k, y=data_poisson)\nax_poisson.set(xlabel='Count', ylabel='Density')\nplt.xticks(rotation=90);\n\n\n\n\n\n\n\n\nAs mentioned before the poisson is the most common distribution we use to model counts (events, people, animals, etc.). However, we are constrained by the fact that the mean and variance are the same. This means that more counts, and as a result larger means allow for greater variation, whereas fewer counts with a smaller means allow for less variation. If we need to allow for more variation (i.e. wider distribution of counts) we can look to the negative binomial.\n\nFun Fact: One of the most famous historical, practical uses of the Poisson distribution was estimating the annual number of Prussian cavalry soldiers killed due to horse-kicks.\n\n\n\n\nThe negative binomial distribution is discrete, like the Poisson, but its variance can be larger than its mean, (i.e. it can be overdispersed). It is often used when the data largely follows a Poisson distribution but there are a large number of zeros in the data (e.g. few sightings and a large number of absences).\nIn fact, the negative binomial distribution is a generalisation of the poisson distribution, whose variance is considered separately from its mean, and hence works well when data are overdispersed. The negative binomial is usually characterised in similar terms to the binomial distribution - \\(r\\) (number of failures observed) and \\(p\\) (probability of success), with \\(k\\) now representing the probability of seeing \\(k\\) successes before \\(r\\) failures are observed given \\(p\\).\nThis is how it is done in scipy.stats, however it can also be parameterised in terms of its mean and variance. As a result, the mean and variance do not have to be equal as with the Poisson distribution, which makes it more flexible.\n\\[p = \\frac{\\sigma^{2} - \\mu}{\\sigma^{2}}\\]\n\\[r = \\frac{\\mu^{2}}{\\sigma^{2} - \\mu}\\]\nGiving the Negative Binomial distribution as:\n\\[P(X = k) = \\left(\n    \\begin{matrix}\n        k + r - 1\\\\\n        k\n    \\end{matrix}\n\\right)p^{k} \\left( \\frac{\\mu}{\\sigma^{2}}  \\right)^{r} \\]\nWhere:\n\\(p\\) is the probability of success in a Bernoulli trial with two outcomes (success or failure);\n\\(r\\) is the dispersion or shape parameter;\n\\(k\\) is the given number of successes of interest.\nNote that although commonly called \\(r\\), scipy refers to this parameter as \\(n\\).\nIn scipy.stats, nbinom takes n and p as shape parameters where n is the number of successes, whereas p is the probability of a single success.\nIn the code below we are showing a different approach, in this case we are generating random values from the negative binomial distribution. We are plotting on the x axis the number of successes (e.g. these could be counts of animals or numbers of people) before we get our first failure (e.g. no animal or no person detected).\nIn this case size is the number of times we run the trial. n is the number of successes before a failure, and p is the probability of a success.\n\n# generate the data from the negative binomial distribution\ndata_nbinom = nbinom.rvs(n=5, p=0.5, size=1000)\n\n# plot the distribution\nax_nbinom = sns.distplot(data_nbinom, kde=False)\n\n# add labels\nax_nbinom.set(xlabel='Negative Binomial', ylabel='Frequency')\nax_nbinom;\n\nC:\\Users\\tbalb\\AppData\\Local\\Temp\\ipykernel_1884\\2654644825.py:5: UserWarning:\n\n\n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n\n\n\n\n\n\n\n\n\n\nFrom this we can see that 4 successes before we see a failure is the most frequent value, followed by 5 successes. You’ll notice that we also get a frequency for 0 to 30 successes. We can transform these into probabilities by dividing the frequency by the size (number of trials) we ran.\nThe negative binomial, may perhaps seem a little abstract to relate the number of successes before a failure to counts, but the important information to take away is that the negative binomial can be an effective way to model count data that is overdispersed (i.e. the data has a larger variance than the mean).",
    "crumbs": [
      "3 - Generalised Linear Models"
    ]
  },
  {
    "objectID": "course_content/Chapter 3 - Generalised Linear Models.html#exercises",
    "href": "course_content/Chapter 3 - Generalised Linear Models.html#exercises",
    "title": "Chapter 3 -Generalised Linear Models",
    "section": "",
    "text": "Question 1Answer\n\n\n\nReflect on some examples from your own work. Write a short paragraph or discuss within a group of where you have a response variable that comes from a Binomial, Poisson, Negative binomial or Normal distribution?\nExperiment with the poisson distribution. What happens when you experiment with the shape parameter, mu?\nExperiment with the normal distribution. What happens when you change the variance? What happens when you change the mean?\n\n\n\n\nYour examples will vary depend on your area of work. Response variables that come from a binomial distribution might include a model where you are looking at two outcomes e.g. presence/absence, dead/alive etc. A Poisson response variable will typically be a count, e.g. how many bicycles crossed the Brooklyn Bridge, numbers of wildebeest, number of times a mobile phone game has been played in a day. These are discrete counts, you can’t have 4.5 of a wildebeest or 5.5 bicycles crossing the Brookly bridge. Response variables with a negative binomial distribution are a bit harder to detect, but they might include outcomes where you have a lot of zeros, e.g. if you typically only see pods of whales and have a lot of instances with zero whales observed, or where the data is more variable some very high counts and some low counts. Normally distributed response variable might include measurements such as weight or height. It is typically used for continuous variables.\nExperiment with the poisson distribution. What happens when you experiment with the shape parameter, mu? We observe that the Poisson distributions are unimodal (one peak); exhibit positive skew (that decreases as mu increases); are centred roughly on mu; have variance (spread) that increases as mu increases.\nExperiment with the normal distribution. What happens when you change the variance? What happens when you change the mean? Changing the mean will shift where the normal distribution is centered. Increasing the variance will widen the distribution (more flat), decreasing the variance will tighten the distribution (more peaked).",
    "crumbs": [
      "3 - Generalised Linear Models"
    ]
  },
  {
    "objectID": "course_content/Chapter 3 - Generalised Linear Models.html#implementing-generalized-linear-models-in-python",
    "href": "course_content/Chapter 3 - Generalised Linear Models.html#implementing-generalized-linear-models-in-python",
    "title": "Chapter 3 -Generalised Linear Models",
    "section": "",
    "text": "A generalized linear model consists of 3 parts:\n\nAn exponential family probability distribution\nA linear predictor\nA link function\n\nThe exponential family probability distribution, is the probability distribution that our response variable follows, for instance, Normal, Binomial, Poisson, and Negative Binomial.\nThe linear predictor is a linear combination of a set of coefficients and explanatory variables used to predict our response variable. This is essentially the same form used to specify our linear model, which means we can use the same formula set up for our GLM.\nThe link function allows us to use the linear predictor by providing the link to connect the exponential distribution to the linear predictor. There will often be defaults for the link when you call the model family, but you can also specify this yourself. For instance, the Poisson distribution is a discrete distribution where values can take on integers between 0 and infinity. However, the linear predictor is based on the normal distribution that is continuous and can go from -infinity to infinity. It is therefore common to use a log link to log the response variable, which effectively limits the normal distribution to 0 to infinity, thereby making it suitable for the Poisson distribution. This means that we often need to do a back transformation when we interpret the parameter coefficients in the actual scale. For example, when using the log-link we would exponentiate the coefficients.\nSpecifying the exponential family\nTo specify the exponential family we add an additional argument in our model specification, family = sm.families.Binomial(). There are several families we can specify, here are the main ones we have covered so far:\n\nsm.families.Binomial()\nsm.families.Gaussian() *i.e. normal distribution\nsm.families.Poisson()\nsm.families.NegativeBinomial()\n\nYou change the link from the default, according to the model you wish to fit, by specifying it as follows:\n\nsm.families.Binomial(link='probit')\n\nWe then specify the formula the same as before (y ~ x).\nformula = 'y ~ x'\nmod1 = smf.GLM(formula = formula, data = df, family = sm.families.Binomial()).fit()\nmod1.summary()",
    "crumbs": [
      "3 - Generalised Linear Models"
    ]
  },
  {
    "objectID": "course_content/Chapter 3 - Generalised Linear Models.html#the-poisson-distribution",
    "href": "course_content/Chapter 3 - Generalised Linear Models.html#the-poisson-distribution",
    "title": "Chapter 3 -Generalised Linear Models",
    "section": "4.1 The Poisson Distribution",
    "text": "4.1 The Poisson Distribution\nThe Poisson distribution expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known constant rate and independently of the time since the last event. The Poisson distribution can also be used for the number of events in other specified intervals such as distance, area or volume.\nThe Poisson distribution is given by the formula:\n\\[P(k\\ \\mathrm{events}) = \\frac{\\lambda^{k}e^{-\\lambda}}{k!} \\]\nWhere, \\(\\lambda\\) is the average density of the events (e.g. cyclists per day), and the function returns the likelihood of seeing \\(k\\) cyclists.\nIn some cases, such as in python, \\(\\lambda\\) (lambda) is refered to as \\(\\mu\\) (mu).\nFor bicycle counts, it allows us to generate estimates for the number of cyclists crossing the Brooklyn Bridge on any given day. We can do this by taking the mean number of cyclists per day, 2680, and by generating 10,000 samples from the poisson distribution:\n\n# generate the counts of the number of cyclists\n# mu is mean cyclists per day, size is 10000 samples.\nsim_counts = poisson.rvs(mu=2680, size=10000)\n\n# plot the counts through time\nf, ax = plt.subplots(figsize=(6, 5))\nax.hist(sim_counts, bins=20, color='aquamarine')\n\n# add the line representing the mean counts\nax.axvline(sim_counts.mean(), linestyle='dashed', color='goldenrod')\n\n# add labels\nax.set_ylabel('Frequency')\nax.set_xlabel('Simulated Daily Number of Bicycles crossing the Brooklyn Bridge');\n\n\n\n\n\n\n\n\nAt first glance this looks reasonable, we’ve drawn a peaked distribution around our chosen \\(\\lambda\\). However, look a bit closer and you’ll see something is not right. The spread of the simulated poisson data doesn’t seem to be any where near the spread of our actual data.\nNotice that the Poisson distribution only has one parameter: \\(\\lambda\\) (aka \\(\\mu\\)), this parameter tells us where the peak of the distribution is located on the x axis. By comparison, the normal distribution has a similar parameter, \\(\\mu\\), which tells you where to locate the peak of the normal distribution. The normal distribution also has another parameter, \\(\\sigma\\), the standard deviation which specifies the spread of the data around \\(\\mu\\). Because the poisson distribution only has \\(\\lambda\\), an assumption is made that the variance, \\(\\sigma^2\\), of the data is equal to the mean (\\(\\lambda\\)), this assumption is called the “equi-dispersion” assumption. Effectively, for a poisson model to be appropriate, the variance = mean.\nWe can see that in the simulated data we produced above this is the case, the effect of having the variance equal to the mean is that the standard deviation, \\(\\sigma\\), of the count data is only 52 bicycles per day (\\(\\sqrt(2680)\\)). The actual data has a much greater spread and as a result a much higher variance.\n\n# mean and variance of simulated poisson data\nsim_counts.mean(), sim_counts.var()\n\n(2680.8564, 2701.46177904)\n\n\n\n# mean and variance of cyclist data\nbikes['bb_count'].mean(), bikes['bb_count'].var()\n\n(2680.042056074766, 730530.6601948135)\n\n\nWhile the simulated data obeys the equi-dispersion assumption, it is clear that our observed data does not. While we can happily fit a poisson regression to model our cyclist counts, the model will violate the equi-dispersion assumption and produce artificially small standard errors. This could lead to you attributing significance to model coefficients that aren’t, in reality, important.",
    "crumbs": [
      "3 - Generalised Linear Models"
    ]
  },
  {
    "objectID": "course_content/Chapter 3 - Generalised Linear Models.html#the-negative-binomial-distribution",
    "href": "course_content/Chapter 3 - Generalised Linear Models.html#the-negative-binomial-distribution",
    "title": "Chapter 3 -Generalised Linear Models",
    "section": "4.2 The Negative Binomial Distribution",
    "text": "4.2 The Negative Binomial Distribution\nThe negative binomial distribution is a generalisation of the poisson distribution, whose variance is considered separately from its mean, and hence works well when data are overdispersed. The negative binomial is usually characterised in similar terms to the binomial distribution: \\(r\\) (number of failures observed) and \\(p\\) (probability of success), with \\(k\\) now representing the probability of seeing \\(k\\) successes before \\(r\\) failures are observed given \\(p\\).\nThis is how it is done in scipy.stats, however it can also be parameterised in terms of its mean and variance. As a result, the two do not have to be equal as with the Poisson distribution, and the negative binomial distribution can better represent the count data of interest.\n\\[p = \\frac{\\sigma^{2} - \\mu}{\\sigma^{2}}\\]\n\\[r = \\frac{\\mu^{2}}{\\sigma^{2} - \\mu}\\]\nGiving the Negative Binomial distribution as:\n\\[P(X = k) = \\left( \\begin{matrix}\nk + r - 1\\\\\nk\n\\end{matrix} \\right)p^{k} \\left( \\frac{\\mu}{\\sigma^{2}}  \\right)^{r} \\]\nWhere:\n\\(p\\) is the probability of success in a Bernoulli trial with two outcomes (success or failure);\n\\(r\\) is the dispersion or shape parameter;\n\\(k\\) is the given number of successes of interest.\nWe can parameterise a negative binomial distribution based on the mean and variance of the observed bicycle count data.\nNote that although commonly called \\(r\\), scipy refers to this parameter as \\(n\\).\nWe can calculate these values directly from the data using:\n\n# Calculate the probability of success, p, in a Bernoulli trial\np = (bikes['bb_count'].var() - bikes['bb_count'].mean()) / bikes['bb_count'].var()\np\n\n0.9963313763513224\n\n\n\n# Calculate the dispersion or shape parameter, r. \nr = (bikes['bb_count'].mean() **2) / (bikes['bb_count'].var() - bikes['bb_count'].mean())\nr\n\n9.868268630034121\n\n\n\nf, ax = plt.subplots(figsize=(8, 7))\n\n# plot the counts of the data\nax.hist(bikes['bb_count'],\n        color='darkblue',\n        bins=15,\n        alpha=0.5,\n        density=True,\n        label='Observed Counts')\n\n# plot a negative binomial using the derived coefficient values\nax.plot(nbinom.pmf(np.arange(0, 5000), n=r, p=1-p),\n        color='darkorange',\n        label=\"Negative Binomial\")\n\n# plot a normal distribution using the mean and standard deviation from the data\n# not that we are using standard deviation not variance for this model\nax.plot(norm.pdf(np.arange(0, 5000), loc=bikes['bb_count'].mean(), scale=bikes['bb_count'].std()),\n        color='cyan',\n        label='Normal')\n\nax.set_xlabel(\"Daily Number of Bicycles crossing the Brooklyn Bridge\")\nax.set_ylabel(\"Density\")\nax.legend();\n\n\n\n\n\n\n\n\nThe negative binomial model is not an exact fit for the bicycle count data, however we don’t require it to be. We simply assume errors will follow a negative binomial distribution, the count of bicycle crossings can be a mix of distributions to be dealt with in the modelling phase to come.\nThe normal distribution is also often seen as a reasonable approximation for count data when the average value for count is high, the normal curve is also superimposed on the graph.\nIn general, the assumption that each ‘event’ (e.g. bicycle crossing the Brookyln Bridge) is independent is likely to be unlikely, but in the absence of any other information we’ll make this assumption.",
    "crumbs": [
      "3 - Generalised Linear Models"
    ]
  },
  {
    "objectID": "course_content/Chapter 3 - Generalised Linear Models.html#what-do-my-predictions-look-like",
    "href": "course_content/Chapter 3 - Generalised Linear Models.html#what-do-my-predictions-look-like",
    "title": "Chapter 3 -Generalised Linear Models",
    "section": "6.1 What do my predictions look like?",
    "text": "6.1 What do my predictions look like?\nWe can also have a look at the quality of our predictions by plotting them against the observed values.\nUsefully, when we predict values, statsmodels does all the exponentiation for us, so we don’t have to worry about the logged coefficients!\n\nf, ax = plt.subplots(figsize=(8, 6))\n\nbikes['predict'] = poisson_glm.mu\n\n# plot counts over time\nax.plot(bikes['date'], bikes['bb_count'], label='Observed')\n\n# plot predicted count over time\nax.plot(bikes['date'], bikes['predict'], label='Predicted')\n\n# add legend and labels\nax.legend()\nax.set_ylabel(\"Daily Number of Bicycles\")\nax.set_xlabel(\"Time\");\n\n\n\n\n\n\n\n\nThe model currently only predicts means for weekdays and weekends, clearly there is more to account for if we want to make reasonable or useful predictions for cyclist behaviour on the Brooklyn Bridge. As we are missing the peaks and there is a lot of variation that is unaccounted for.\nIn statsmodels, the same results can be achieved using the Poisson regression model from the discrete dependent variable submodule directly:\n\npoisson_model = sm.formula.poisson(formula='bb_count~weekend', \n                                   data=bikes).fit()\npoisson_model.summary()\n\nOptimization terminated successfully.\n         Current function value: 163.507016\n         Iterations 4\n\n\n\nPoisson Regression Results\n\n\nDep. Variable:\nbb_count\nNo. Observations:\n214\n\n\nModel:\nPoisson\nDf Residuals:\n212\n\n\nMethod:\nMLE\nDf Model:\n1\n\n\nDate:\nMon, 09 Dec 2024\nPseudo R-squ.:\n0.02919\n\n\nTime:\n15:33:27\nLog-Likelihood:\n-34991.\n\n\nconverged:\nTrue\nLL-Null:\n-36043.\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n0.000\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n7.9312\n0.002\n5158.154\n0.000\n7.928\n7.934\n\n\nweekend[T.True]\n-0.1362\n0.003\n-45.393\n0.000\n-0.142\n-0.130\n\n\n\n\n\nThe model produces the same output as the GLM approach, however the output is a little different - we have the pseudo-\\(R^{2}\\) value now instead of the deviance and the Pearson \\(\\chi^{2}\\) statistic. We also get a likelihood ratio test, telling us that the fitted model is an improvement over the intercept-only ‘null’ model.",
    "crumbs": [
      "3 - Generalised Linear Models"
    ]
  },
  {
    "objectID": "course_content/Chapter 3 - Generalised Linear Models.html#is-my-poisson-model-overdispersed",
    "href": "course_content/Chapter 3 - Generalised Linear Models.html#is-my-poisson-model-overdispersed",
    "title": "Chapter 3 -Generalised Linear Models",
    "section": "6.2 Is my Poisson model overdispersed?",
    "text": "6.2 Is my Poisson model overdispersed?\nAs we’ve noted, the Poisson distribution assumes that the given mean is equal to the variance. If, in fact, our variance proves to be larger than the mean, we have overdispersed count data, which is inappropriate for the poisson model. Overdispersed data are inappropriate because they lead to an underestimation of the standard errors of our regression coefficients (these represent the precision or uncertainty of our coefficient estimates). Understated standard errors may lead us to believe that particular explanatory variables are significant in explaining our outcome of interest, when in reality they are not.\nA simple check for overdispersion is the dispersion statistic, which is given by:\n\\[\\frac{\\mathrm{Pearson}\\ \\chi^{2}}{\\mathrm{Residual\\ Degrees\\ of\\ Freedom}}\\]\nWhen the data is drawn from a Poisson distribution, this value should be approximately 1. A ratio less than 1 implies underdispersion (i.e. variance &lt; mean), and a value more than 1 implies overdispersion (i.e. variance &gt; mean).\nData that is underdispersed often arises dues to excess zeros, resulting from a separate data generating mechanism and requires a zero-inflated model which first fits a logit/probit model for zeroes vs. counts, and then a count model on the counts.\nData that is overdispersed can be modelled using a negative binomial model or a quasi-poisson approach.\n\ndispersion_stat = poisson_glm.pearson_chi2 / poisson_glm.df_resid\ndispersion_stat\n\n265.5645452366999\n\n\nClearly from the dispersion statistic we’re deep in overdispersion territory! Let’s look now to fitting a negative binomial model to deal with this!",
    "crumbs": [
      "3 - Generalised Linear Models"
    ]
  },
  {
    "objectID": "course_content/Chapter 3 - Generalised Linear Models.html#example",
    "href": "course_content/Chapter 3 - Generalised Linear Models.html#example",
    "title": "Chapter 3 -Generalised Linear Models",
    "section": "Example",
    "text": "Example\n\nExample 1Example 2 \n\n\n\nnb_mle = sm.formula.negativebinomial(formula='bb_count~weekend', data=bikes).fit()\nnb_mle.summary()\n\nOptimization terminated successfully.\n         Current function value: 8.349478\n         Iterations: 6\n         Function evaluations: 8\n         Gradient evaluations: 8\n\n\n\nNegativeBinomial Regression Results\n\n\nDep. Variable:\nbb_count\nNo. Observations:\n214\n\n\nModel:\nNegativeBinomial\nDf Residuals:\n212\n\n\nMethod:\nMLE\nDf Model:\n1\n\n\nDate:\nMon, 09 Dec 2024\nPseudo R-squ.:\n0.001354\n\n\nTime:\n15:33:27\nLog-Likelihood:\n-1786.8\n\n\nconverged:\nTrue\nLL-Null:\n-1789.2\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n0.02772\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n7.9312\n0.033\n241.759\n0.000\n7.867\n7.995\n\n\nweekend[T.True]\n-0.1362\n0.061\n-2.235\n0.025\n-0.256\n-0.017\n\n\nalpha\n0.1632\n0.015\n10.574\n0.000\n0.133\n0.193\n\n\n\n\n\n\nfrom IPython.display import display\n\n# Create rain categorical\nbikes['rain'] = pd.cut(bikes['precip'], bins = [-1,0.001,0.3,4], labels = ['No Rain', 'Light Rain', 'Heavy Rain'])\n\n# Fit Poisson model\nformula = 'bb_count ~ high_temp_C + rain + weekend'\npoisson_glm_ex1 = sm.formula.glm(formula = formula, data=bikes, family = sm.families.Poisson()).fit()\n\n# Compute alpha\noutcome = ((bikes['bb_count'] - poisson_glm_ex1.mu)**2 - bikes['bb_count'])/ poisson_glm_ex1.mu\naux_ols = sm.OLS(outcome, poisson_glm_ex1.mu).fit()\nalpha = aux_ols.params['x1']\n\n# Fit negative binomial model\nnbin_glm_ex1 = sm.formula.glm(formula=formula, data=bikes, family = sm.families.NegativeBinomial(alpha = alpha)).fit()\ndisplay(nbin_glm_ex1.summary())\n\n# Plot predictions\n\nf, ax = plt.subplots(figsize=(12,6))\nbikes['predict'] = nbin_glm_ex1.fittedvalues\nax.plot(bikes['date'], bikes['bb_count'], label = 'Observed')\nax.plot(bikes['date'], bikes['predict'], label = 'Predicted')\nax.legend()\nax.set_ylabel(\"Daily Number of Bicycles\")\nax.set_xlabel(\"Time\")\nax.set_title(\"Bicycle Counts: Observed vs. Negative Binomial Regression Model Predictions\");\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\nbb_count\nNo. Observations:\n214\n\n\nModel:\nGLM\nDf Residuals:\n209\n\n\nModel Family:\nNegativeBinomial\nDf Model:\n4\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-1738.2\n\n\nDate:\nMon, 09 Dec 2024\nDeviance:\n430.44\n\n\nTime:\n15:33:27\nPearson chi2:\n376.\n\n\nNo. Iterations:\n8\nPseudo R-squ. (CS):\n0.9003\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n7.3790\n0.063\n118.022\n0.000\n7.256\n7.502\n\n\nrain[T.Light Rain]\n-0.1833\n0.031\n-5.850\n0.000\n-0.245\n-0.122\n\n\nrain[T.Heavy Rain]\n-0.8789\n0.046\n-18.980\n0.000\n-0.970\n-0.788\n\n\nweekend[T.True]\n-0.1142\n0.030\n-3.804\n0.000\n-0.173\n-0.055\n\n\nhigh_temp_C\n0.0276\n0.002\n11.361\n0.000\n0.023\n0.032\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom IPython.display import display\n\n# Create rain categorical\nbikes['rain'] = pd.cut(bikes['precip'], bins = [-1,0.001,0.3,4], labels = ['No Rain', 'Light Rain', 'Heavy Rain'])\n\n# Fit Poisson model\nformula = 'bb_count ~ rain*high_temp_C + weekend'\npoisson_glm_ex1 = sm.formula.glm(formula = formula, data=bikes, family = sm.families.Poisson()).fit()\n\n# Compute alpha\noutcome = ((bikes['bb_count'] - poisson_glm_ex1.mu)**2 - bikes['bb_count'])/ poisson_glm_ex1.mu\naux_ols = sm.OLS(outcome, poisson_glm_ex1.mu).fit()\nalpha = aux_ols.params['x1']\n\n# Fit negative binomial model\nnbin_glm_ex1 = sm.formula.glm(formula=formula, data=bikes, family = sm.families.NegativeBinomial(alpha = alpha)).fit()\ndisplay(nbin_glm_ex1.summary())\n\n# Plot predictions\n\nf, ax = plt.subplots(figsize=(12,6))\nbikes['predict'] = nbin_glm_ex1.fittedvalues\nax.plot(bikes['date'], bikes['bb_count'], label = 'Observed')\nax.plot(bikes['date'], bikes['predict'], label = 'Predicted')\nax.legend()\nax.set_ylabel(\"Daily Number of Bicycles\")\nax.set_xlabel(\"Time\")\nax.set_title(\"Bicycle Counts: Observed vs. Negative Binomial Regression Model Predictions\");\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\nbb_count\nNo. Observations:\n214\n\n\nModel:\nGLM\nDf Residuals:\n207\n\n\nModel Family:\nNegativeBinomial\nDf Model:\n6\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-1710.9\n\n\nDate:\nMon, 09 Dec 2024\nDeviance:\n398.07\n\n\nTime:\n15:33:28\nPearson chi2:\n358.\n\n\nNo. Iterations:\n7\nPseudo R-squ. (CS):\n0.9461\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n7.6958\n0.080\n96.160\n0.000\n7.539\n7.853\n\n\nrain[T.Light Rain]\n-0.4470\n0.124\n-3.608\n0.000\n-0.690\n-0.204\n\n\nrain[T.Heavy Rain]\n-2.2433\n0.151\n-14.834\n0.000\n-2.540\n-1.947\n\n\nweekend[T.True]\n-0.1344\n0.029\n-4.703\n0.000\n-0.190\n-0.078\n\n\nhigh_temp_C\n0.0148\n0.003\n4.650\n0.000\n0.009\n0.021\n\n\nrain[T.Light Rain]:high_temp_C\n0.0108\n0.005\n2.085\n0.037\n0.001\n0.021\n\n\nrain[T.Heavy Rain]:high_temp_C\n0.0623\n0.007\n9.161\n0.000\n0.049\n0.076",
    "crumbs": [
      "3 - Generalised Linear Models"
    ]
  },
  {
    "objectID": "course_content/Chapter 3 - Generalised Linear Models.html#next-steps-other-statistical-models",
    "href": "course_content/Chapter 3 - Generalised Linear Models.html#next-steps-other-statistical-models",
    "title": "Chapter 3 -Generalised Linear Models",
    "section": "8.1 Next Steps: other statistical models",
    "text": "8.1 Next Steps: other statistical models\nIn this course we have covered linear regression and a few examples of generalized linear models. There is a whole range of models we can fit. Some of the topics we haven’t covered is capturing structure in our data (e.g. repeated measures, spatial and temporal correlation, random effects, generalised additive models etc.).\n\nImage Credit: Ecological Models and Data in R by Ben Bolker 2007, pg 397.",
    "crumbs": [
      "3 - Generalised Linear Models"
    ]
  },
  {
    "objectID": "course_content/Chapter 1 - Exploratory Data Analysis.html",
    "href": "course_content/Chapter 1 - Exploratory Data Analysis.html",
    "title": "Chapter 1- Exploratory Data Analysis",
    "section": "",
    "text": "Exploratory data analysis is a fluid process and there is no single approach. It can be thought of a process of hypothesis generation, data exploration, and formal statistical testing. It comes after the stage of importing and tidying your data.\nIn this section we will walk through organising your data, getting to know your data structure, and understanding variation and covariation within and between variables.\nFor these exercises we will use a simulated dataset of departmental salaries, which consists of the salaries of professors in 5 departments: english, informatics, statistics, biology, and sociology.\n\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # tabular data processing\nimport matplotlib.pyplot as plt # data plotting\nimport seaborn as sns # data visualisation and plotting\n\n# Set a style\nplt.style.use('ggplot')\n\n# set the custom size for my graphs\nsns.set(rc={'figure.figsize':(8.7,6.27)})\n\n# supress warnings due to different versions of packages\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# set plots to show inline\n%matplotlib inline\n\n\n\n\nTo conduct regression analyses in python it is important to have your data in a format that is easy to work with. A good approach is to organise your data in a “tidy” format.\nTidy data is a set of values, where each value is placed in its own “cell”, each variable in its own column, and each observation in its own row. Although this course is on Statistics in Python, the book R for Data Science by Garrett Grolemund and Hadley Wickham is a great resource when thinking about tidying data. Here, we use some of the definitions they set out in the book to describe tidy data.\nSome definitions for tidy data:\n\nA variable is a quantity, quality, or property that you can measure.\nA value is the state of a variable when you measure it. The value of a variable may change from measurement to measurement.\nAn observation is a set of measurements made under similar conditions (you usually make all of the measurements in an observation at the same time and on the same object). An observation will contain several values, each associated with a different variable.\nIn python, the pandas package makes it easy to format your data in a tabular format.\n\nReading in the data\n\n# read in the salary data\nsalaries = pd.read_csv(\"../data/faculty-data.csv\") \n\n\n# looking at the head of the dataframe\nsalaries.head()\n\n\n\n\n\n\n\n\nids\ndepartment\nbases\nexperience\nraises\nsalary\n\n\n\n\n0\n1\nsociology\n39012.062997\n3\n2122.325646\n45379.039935\n\n\n1\n2\nbiology\n51872.123941\n9\n541.643975\n56746.919719\n\n\n2\n3\nenglish\n64341.126468\n3\n543.178641\n65970.662390\n\n\n3\n4\ninformatics\n68975.266754\n2\n1736.946839\n72449.160433\n\n\n4\n5\nstatistics\n78262.278702\n9\n469.943148\n82491.767032\n\n\n\n\n\n\n\nExercise\nOur salary data consists of the salaries of individual professors from 5 departments within the University: english, informatics, statistics, biology, and sociology. The data contains the following variables\n\nids = individual id\ndepartment = university department\nbases = the starting salary\nexperience = years of experience\nraises = raise per year\nsalary = current salary\n\nWe’re interested in exploring the relationship between years of experience and salary, but first we need to get to know our data a bit better.\n\nLooking at the definitions for value, observation, and variable, give an example of each term from the salary data.\n\n\n\n\nTo get started, let’s explore the following questions for our dataset.\n\nWhat is the structure of the data?\nWhat type of variation occurs within my variables?\nWhat type of covariation occurs between my variables?\n\n\n\nOne of the things we will wish to know about our variables are whether they are continuous or categorical.\n Image Credit: @AllisonHorst.\n\nContinuous variable: a variable that can take on an unlimited number of values between the lowest and highest points of measurements.\n\ne.g. speed, distance, height\n\nCategorical variable can take one of a limited subset of values. For example, if you have a dataset about a household then you will typically find variables like employment sector, marriage status, and country.\n\nIn python, categorical variables are usually stored as character strings or integers (e.g. ‘Industry’ and ‘Academia’ for types of employment sector).\nCategorical variables are nominal if they have no order (e.g. ‘Ghana’ and ‘Uruguay’)\nCategorical variables are ordinal if there is an order associated with them (e.g. ‘low’, ‘medium’, and ‘high’ referring to economic status).\n\n\nAs we start to think about our data and the types of variables we are working with, we recommend reading Chapter 4: “What Gets Counted Counts” of Data Feminism by Catherine D’Ignazio and Lauren Klein. The book encourages us to rethink binaries and examine classification systems in data science.\n\n\n\n\nQuestion 1 Answer\n\n\nWhat are the dimensions of the dataframe?\n\n\n\n# the shape attribute returns the dimensions of the dataframe\nsalaries.shape\n\n(100, 6)\n\n\n\n\n\n\n\n\n\nQuestion 2Answer\n\n\nWhat are the first and last values of salary?\n\n\n\n# the head method shows us the first 5 rows\n\nsalaries.head()\n\n\n\n\n\n\n\n\nids\ndepartment\nbases\nexperience\nraises\nsalary\n\n\n\n\n0\n1\nsociology\n39012.062997\n3\n2122.325646\n45379.039935\n\n\n1\n2\nbiology\n51872.123941\n9\n541.643975\n56746.919719\n\n\n2\n3\nenglish\n64341.126468\n3\n543.178641\n65970.662390\n\n\n3\n4\ninformatics\n68975.266754\n2\n1736.946839\n72449.160433\n\n\n4\n5\nstatistics\n78262.278702\n9\n469.943148\n82491.767032\n\n\n\n\n\n\n\n\n\n\n\n# the tail method shows us the last 5 rows\n\nsalaries.tail()\n\n\n\n\n\n\n\n\nids\ndepartment\nbases\nexperience\nraises\nsalary\n\n\n\n\n95\n96\nsociology\n42107.716434\n8\n1957.880086\n57770.757125\n\n\n96\n97\nbiology\n49774.735037\n5\n470.338273\n52126.426402\n\n\n97\n98\nenglish\n60919.523254\n3\n502.197549\n62426.115902\n\n\n98\n99\ninformatics\n63809.889673\n7\n1641.660044\n75301.509985\n\n\n99\n100\nstatistics\n84420.476008\n3\n537.931883\n86034.271658\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3Answer\n\n\nWhich variables are categorical or continuous variables? What are these data types called in Python?\n\n\n\n# the info method will tell us about our data frame, including how many observations per column and the type of data.\n\nsalaries.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 100 entries, 0 to 99\nData columns (total 6 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   ids         100 non-null    int64  \n 1   department  100 non-null    object \n 2   bases       100 non-null    float64\n 3   experience  100 non-null    int64  \n 4   raises      100 non-null    float64\n 5   salary      100 non-null    float64\ndtypes: float64(3), int64(2), object(1)\nmemory usage: 4.8+ KB\n\n\n\n\n\nWe often see type: object in pandas, this section of the package user guide explains why this is.\n\n\n\n\nQuestion 4Answer\n\n\nUsing the data summary, what is the minimum and maximum salary?\n\n\n\n# the describe method gives us a summary of the numerical data and the spread of values\n\nsalaries.describe()\n\n\n\n\n\n\n\n\nids\nbases\nexperience\nraises\nsalary\n\n\n\n\ncount\n100.000000\n100.000000\n100.000000\n100.000000\n100.000000\n\n\nmean\n50.500000\n60226.029444\n4.820000\n1033.461045\n65448.393841\n\n\nstd\n29.011492\n14240.030985\n2.889829\n671.385251\n13219.178125\n\n\nmin\n1.000000\n37246.936522\n0.000000\n450.600025\n42827.156913\n\n\n25%\n25.750000\n48071.305546\n3.000000\n481.551932\n54745.711028\n\n\n50%\n50.500000\n59431.965403\n5.000000\n540.378028\n62375.126164\n\n\n75%\n75.250000\n72671.300340\n7.250000\n1741.022583\n78631.780911\n\n\nmax\n100.000000\n87009.876061\n9.000000\n2177.423176\n91342.489460\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 5 Answer\n\n\nWhat are the names of the columns?\n\n\n\n# the columns attribute returns the column names\n\nsalaries.columns\n\nIndex(['ids', 'department', 'bases', 'experience', 'raises', 'salary'], dtype='object')\n\n\n\n\n\n\n\n\n\nQuestion 6Answer\n\n\nDo we have any missing data? How many missing values do we have per variable?\n\n\n\n# the isna() method returns missing values and the sum() method adds them up.\n\nsalaries.isna().sum()\n\nids           0\ndepartment    0\nbases         0\nexperience    0\nraises        0\nsalary        0\ndtype: int64\n\n\n\n\n\nWe can only find missing values when we have told the dataframe what to count as missing data. Sometimes our data may contain unrecognised or other custom values that signify missing values. If this happens we need to overwrite these values as missing values.\n\n\n\n\nQuestion 7 Answer\n\n\nWhat are the unique departments in our dataset?\n\n\n\n# the unique() method returns the distinct values in a column.\n\nsalaries['department'].unique()\n\narray(['sociology', 'biology', 'english', 'informatics', 'statistics'],\n      dtype=object)\n\n\n\n\n\n\n\n\n\nQuestion 8 Answer\n\n\nDo we have any duplicates in our data?\n\n\n\n# the duplicated() method returns duplicates, the sum() method adds them up for us.\n\nsalaries.duplicated().sum()\n\n0\n\n\nNote that if we wanted to filter out duplicates we can use .drop_duplicates(). Although, in this dataset we don’t have duplicates.\n\n# you can filter out the duplicates using the drop_duplicates method.\n\nsalaries.drop_duplicates()\n\n\n\n\n\n\n\n\nids\ndepartment\nbases\nexperience\nraises\nsalary\n\n\n\n\n0\n1\nsociology\n39012.062997\n3\n2122.325646\n45379.039935\n\n\n1\n2\nbiology\n51872.123941\n9\n541.643975\n56746.919719\n\n\n2\n3\nenglish\n64341.126468\n3\n543.178641\n65970.662390\n\n\n3\n4\ninformatics\n68975.266754\n2\n1736.946839\n72449.160433\n\n\n4\n5\nstatistics\n78262.278702\n9\n469.943148\n82491.767032\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n95\n96\nsociology\n42107.716434\n8\n1957.880086\n57770.757125\n\n\n96\n97\nbiology\n49774.735037\n5\n470.338273\n52126.426402\n\n\n97\n98\nenglish\n60919.523254\n3\n502.197549\n62426.115902\n\n\n98\n99\ninformatics\n63809.889673\n7\n1641.660044\n75301.509985\n\n\n99\n100\nstatistics\n84420.476008\n3\n537.931883\n86034.271658\n\n\n\n\n100 rows × 6 columns",
    "crumbs": [
      "1 - Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "course_content/Chapter 1 - Exploratory Data Analysis.html#getting-to-know-your-data",
    "href": "course_content/Chapter 1 - Exploratory Data Analysis.html#getting-to-know-your-data",
    "title": "Chapter 1- Exploratory Data Analysis",
    "section": "",
    "text": "To get started, let’s explore the following questions for our dataset.\n\nWhat is the structure of the data?\nWhat type of variation occurs within my variables?\nWhat type of covariation occurs between my variables?\n\n\n\nOne of the things we will wish to know about our variables are whether they are continuous or categorical.\n Image Credit: @AllisonHorst.\n\nContinuous variable: a variable that can take on an unlimited number of values between the lowest and highest points of measurements.\n\ne.g. speed, distance, height\n\nCategorical variable can take one of a limited subset of values. For example, if you have a dataset about a household then you will typically find variables like employment sector, marriage status, and country.\n\nIn python, categorical variables are usually stored as character strings or integers (e.g. ‘Industry’ and ‘Academia’ for types of employment sector).\nCategorical variables are nominal if they have no order (e.g. ‘Ghana’ and ‘Uruguay’)\nCategorical variables are ordinal if there is an order associated with them (e.g. ‘low’, ‘medium’, and ‘high’ referring to economic status).\n\n\nAs we start to think about our data and the types of variables we are working with, we recommend reading Chapter 4: “What Gets Counted Counts” of Data Feminism by Catherine D’Ignazio and Lauren Klein. The book encourages us to rethink binaries and examine classification systems in data science.\n\n\n\n\nQuestion 1 Answer\n\n\nWhat are the dimensions of the dataframe?\n\n\n\n# the shape attribute returns the dimensions of the dataframe\nsalaries.shape\n\n(100, 6)\n\n\n\n\n\n\n\n\n\nQuestion 2Answer\n\n\nWhat are the first and last values of salary?\n\n\n\n# the head method shows us the first 5 rows\n\nsalaries.head()\n\n\n\n\n\n\n\n\nids\ndepartment\nbases\nexperience\nraises\nsalary\n\n\n\n\n0\n1\nsociology\n39012.062997\n3\n2122.325646\n45379.039935\n\n\n1\n2\nbiology\n51872.123941\n9\n541.643975\n56746.919719\n\n\n2\n3\nenglish\n64341.126468\n3\n543.178641\n65970.662390\n\n\n3\n4\ninformatics\n68975.266754\n2\n1736.946839\n72449.160433\n\n\n4\n5\nstatistics\n78262.278702\n9\n469.943148\n82491.767032\n\n\n\n\n\n\n\n\n\n\n\n# the tail method shows us the last 5 rows\n\nsalaries.tail()\n\n\n\n\n\n\n\n\nids\ndepartment\nbases\nexperience\nraises\nsalary\n\n\n\n\n95\n96\nsociology\n42107.716434\n8\n1957.880086\n57770.757125\n\n\n96\n97\nbiology\n49774.735037\n5\n470.338273\n52126.426402\n\n\n97\n98\nenglish\n60919.523254\n3\n502.197549\n62426.115902\n\n\n98\n99\ninformatics\n63809.889673\n7\n1641.660044\n75301.509985\n\n\n99\n100\nstatistics\n84420.476008\n3\n537.931883\n86034.271658\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3Answer\n\n\nWhich variables are categorical or continuous variables? What are these data types called in Python?\n\n\n\n# the info method will tell us about our data frame, including how many observations per column and the type of data.\n\nsalaries.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 100 entries, 0 to 99\nData columns (total 6 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   ids         100 non-null    int64  \n 1   department  100 non-null    object \n 2   bases       100 non-null    float64\n 3   experience  100 non-null    int64  \n 4   raises      100 non-null    float64\n 5   salary      100 non-null    float64\ndtypes: float64(3), int64(2), object(1)\nmemory usage: 4.8+ KB\n\n\n\n\n\nWe often see type: object in pandas, this section of the package user guide explains why this is.\n\n\n\n\nQuestion 4Answer\n\n\nUsing the data summary, what is the minimum and maximum salary?\n\n\n\n# the describe method gives us a summary of the numerical data and the spread of values\n\nsalaries.describe()\n\n\n\n\n\n\n\n\nids\nbases\nexperience\nraises\nsalary\n\n\n\n\ncount\n100.000000\n100.000000\n100.000000\n100.000000\n100.000000\n\n\nmean\n50.500000\n60226.029444\n4.820000\n1033.461045\n65448.393841\n\n\nstd\n29.011492\n14240.030985\n2.889829\n671.385251\n13219.178125\n\n\nmin\n1.000000\n37246.936522\n0.000000\n450.600025\n42827.156913\n\n\n25%\n25.750000\n48071.305546\n3.000000\n481.551932\n54745.711028\n\n\n50%\n50.500000\n59431.965403\n5.000000\n540.378028\n62375.126164\n\n\n75%\n75.250000\n72671.300340\n7.250000\n1741.022583\n78631.780911\n\n\nmax\n100.000000\n87009.876061\n9.000000\n2177.423176\n91342.489460\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 5 Answer\n\n\nWhat are the names of the columns?\n\n\n\n# the columns attribute returns the column names\n\nsalaries.columns\n\nIndex(['ids', 'department', 'bases', 'experience', 'raises', 'salary'], dtype='object')\n\n\n\n\n\n\n\n\n\nQuestion 6Answer\n\n\nDo we have any missing data? How many missing values do we have per variable?\n\n\n\n# the isna() method returns missing values and the sum() method adds them up.\n\nsalaries.isna().sum()\n\nids           0\ndepartment    0\nbases         0\nexperience    0\nraises        0\nsalary        0\ndtype: int64\n\n\n\n\n\nWe can only find missing values when we have told the dataframe what to count as missing data. Sometimes our data may contain unrecognised or other custom values that signify missing values. If this happens we need to overwrite these values as missing values.\n\n\n\n\nQuestion 7 Answer\n\n\nWhat are the unique departments in our dataset?\n\n\n\n# the unique() method returns the distinct values in a column.\n\nsalaries['department'].unique()\n\narray(['sociology', 'biology', 'english', 'informatics', 'statistics'],\n      dtype=object)\n\n\n\n\n\n\n\n\n\nQuestion 8 Answer\n\n\nDo we have any duplicates in our data?\n\n\n\n# the duplicated() method returns duplicates, the sum() method adds them up for us.\n\nsalaries.duplicated().sum()\n\n0\n\n\nNote that if we wanted to filter out duplicates we can use .drop_duplicates(). Although, in this dataset we don’t have duplicates.\n\n# you can filter out the duplicates using the drop_duplicates method.\n\nsalaries.drop_duplicates()\n\n\n\n\n\n\n\n\nids\ndepartment\nbases\nexperience\nraises\nsalary\n\n\n\n\n0\n1\nsociology\n39012.062997\n3\n2122.325646\n45379.039935\n\n\n1\n2\nbiology\n51872.123941\n9\n541.643975\n56746.919719\n\n\n2\n3\nenglish\n64341.126468\n3\n543.178641\n65970.662390\n\n\n3\n4\ninformatics\n68975.266754\n2\n1736.946839\n72449.160433\n\n\n4\n5\nstatistics\n78262.278702\n9\n469.943148\n82491.767032\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n95\n96\nsociology\n42107.716434\n8\n1957.880086\n57770.757125\n\n\n96\n97\nbiology\n49774.735037\n5\n470.338273\n52126.426402\n\n\n97\n98\nenglish\n60919.523254\n3\n502.197549\n62426.115902\n\n\n98\n99\ninformatics\n63809.889673\n7\n1641.660044\n75301.509985\n\n\n99\n100\nstatistics\n84420.476008\n3\n537.931883\n86034.271658\n\n\n\n\n100 rows × 6 columns",
    "crumbs": [
      "1 - Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "course_content/Chapter 1 - Exploratory Data Analysis.html#visualising-distributions",
    "href": "course_content/Chapter 1 - Exploratory Data Analysis.html#visualising-distributions",
    "title": "Chapter 1- Exploratory Data Analysis",
    "section": "2.1 Visualising Distributions",
    "text": "2.1 Visualising Distributions\nHow you visualise your variables depends on if the variable is categorical or continuous.\nA categorical variable\n\nCategorical or discrete variable: a variable that can take on one of a limited, usually fixed number of possible values, assigning each value to a particular group or nominal category.\n\ne.g. employment sector, economic status, density: (high, medium, low)\n\n\nTo examine the distribution of a categorical variable, we can use a bar plot:\n\nBar plots are a useful tool for getting to know how many observations are within each group of a category.\n\n\n# We can use the countplot method to create a bar chart.\n# We need to specify the variable to plot, department, and the data: salaries.\n\ndepartment_counts = sns.countplot(x=\"department\",\n                                  data=salaries)\ndepartment_counts;\n\n\n\n\n\n\n\n\nIn this case, the bar chart shows that there are the same number of individuals who we have salary data for per department in the data set. A continuous variable * A continuous variable can take any of an infinite set of ordered values (e.g. numbers and date times). We can inspect the spread of the data using a density plot or box plot.\n\n# We can use the distplot method to make a plot of the distribution.\n# We again specify the variable to plot, salary.\n# We can customise the plot using several parameters\n# to include a histogram or not, e.g. hist=False\n# to plot a Gaussian kernel density e.g. kde=True\n# to customise the appearance of the plot, kde_kws,\n# including keyword arguments for underlying plotting functions like shade.\n\nsalary_all_distplot = sns.distplot(salaries['salary'],\n                                   hist=False,\n                                   kde=True,\n                                   kde_kws={'shade': True,\n                                            'linewidth': 3})\n\n# We can then set the xlabel and ylabel for the plot using the method set()\n\nsalary_all_distplot.set(xlabel='Salary',\n                        ylabel='Density');\n\nC:\\Users\\tbalb\\AppData\\Local\\Temp\\ipykernel_19872\\3695421610.py:9: UserWarning:\n\n\n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n\n\n\n\n\n\n\n\n\n\nIf we look at the distribution of salary, something interesting seems to be happening. It appears that the distribution is bimodal meaning that there are two modes, in this case two maxima (around 55,000 USD and 80,000 USD), in the data.\nLet’s explore the data in more detail by plotting the salary by department\n\n# We can explore the distribution of salaries broken down by department\n# Each of the distributions are slightly different shaped\n# There are clear groupings between different departments\n\nsns.displot(data=salaries, x='salary', hue='department', kind='kde')",
    "crumbs": [
      "1 - Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "course_content/Chapter 1 - Exploratory Data Analysis.html#exercises-8",
    "href": "course_content/Chapter 1 - Exploratory Data Analysis.html#exercises-8",
    "title": "Chapter 1- Exploratory Data Analysis",
    "section": "Exercises",
    "text": "Exercises\n\nQuestion 9 Answer\n\n\nMake a box plot or a violin plot of experience by department.\n\n\n\n%load \"./Solutions/ex1.1.py\"",
    "crumbs": [
      "1 - Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "course_content/Chapter 1 - Exploratory Data Analysis.html#exercises-9",
    "href": "course_content/Chapter 1 - Exploratory Data Analysis.html#exercises-9",
    "title": "Chapter 1- Exploratory Data Analysis",
    "section": "Exercises",
    "text": "Exercises\n\nQuestion 10 Answer\n\n\n\nMake a scatterplot to visualise the relationship between base salary bases and raises raises coloured by department. What patterns can you pick out from the data?\n\n\n\n\n%load \"./Solutions/ex1.2.py\"\n\n\n\n\n\nPairplots can be a quick and useful way to summarise your dataset quickly and to inspect the relationships simultaneously.Trying running the following code to make a pairplot. What does this code do?\nUsing SHIFT + TAB to look at the documentation, what other options can you choose for the diagonal plot (diag_kind)?\n\n\n# Using the method pairplot we can explore all variables in our data set.\n# We need to specify the data, and we can add a colour with the hue parameter.\n# We can choose what plot is on the diagonal using the parameter diag_kind \n# (e.g. kde for Gaussian kernel density) like the one we made using distplot. \n\n\nsalaries_pairplot = sns.pairplot(data=salaries, \n                                 hue=\"department\", \n                                 diag_kind=\"kde\")\nsalaries_pairplot;\n\n\n\n\n\n\n\n\nExploratory Data Analysis is a useful tool to identify and pick out patterns to explore, but we need to confirm any results with statistical analyses.",
    "crumbs": [
      "1 - Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "course_content/Chapter 2 - Model Basics.html",
    "href": "course_content/Chapter 2 - Model Basics.html",
    "title": "Chapter 2 -Model Basics",
    "section": "",
    "text": "Ideally your model will capture signals (i.e. patterns) generated by the phenomenom of interest and ignore noise (i.e. random variation) you’re not interested in.\nIn model basics you will learn how models work mechanistically, focussing on the important family of linear models.\n\nHypothesis generation vs. hypothesis confirmation\n\nTraditionally, one of the focuses of modelling is on inference, or confirming that a hypothesis is true. To do it correctly you need to know two things:\n\nEach observation can either be used for exploration or confirmation, not both.\nYou can use an observation as many times as you like for exploration, but you can only use it once for confirmation. As soon as you use an observation twice, you’ve switched from confirmation to exploration.\n\nNote, that in order to confirm a hypothesis, you must use data independent of the data used to generate the hypothesis.",
    "crumbs": [
      "2 - Model Basics"
    ]
  },
  {
    "objectID": "course_content/Chapter 2 - Model Basics.html#what-influences-a-professors-salary",
    "href": "course_content/Chapter 2 - Model Basics.html#what-influences-a-professors-salary",
    "title": "Chapter 2 -Model Basics",
    "section": "4.1 What influences a professor’s salary?",
    "text": "4.1 What influences a professor’s salary?\nIn this chapter, we’re going to explore the question, what influences a professor’s salary?\nOur data consists of the salaries of individual professors from 5 departments within the University: english, informatics, statistics, biology, and sociology. The data contains the following variables\n\nids = individual id\ndepartment = university department\nbases = the starting salary\nexperience = years of experience\nraises = raise per year\nsalary = current salary\n\nWe’re interested in exploring the questions “What influences a professor’s salary?”. We have a hunch that it might depend on years of experience and that it could be different across departments.\n\n# read in the salary data\nsalaries = pd.read_csv(\"../data/faculty-data.csv\") \n\n\n# Using the scatterplot function we can create a scatterplot of salary and experience\n# to explore this relationship with department in more detail.\n\nsalaries_scatter = sns.scatterplot(data=salaries,\n                                   x='experience',\n                                   y='salary',\n                                   hue='department')\nplt.legend(loc='upper right');",
    "crumbs": [
      "2 - Model Basics"
    ]
  },
  {
    "objectID": "course_content/Chapter 2 - Model Basics.html#exercises",
    "href": "course_content/Chapter 2 - Model Basics.html#exercises",
    "title": "Chapter 2 -Model Basics",
    "section": "Exercises",
    "text": "Exercises\n\nQuestion 1Answer\n\n\nWhat kind of relationship do you see between years of experience and salary? How does salary change with years of experience? Does it vary by department?\n\n\n\n# From the plot it looks like there is a positive relationship between the salary an individual receives and the years of experience they have. That is as the years of experience they have increases the salary also increases. \n\n# While the relationships looks positive within departments, the strength of the relationships look different for each department. \n\n\n\n\nLet’s use a model to capture the pattern and make it more explicit. In this case, the relationship looks linear so we can use the form: \\(y = ax + b\\). We are interested in how an individual’s salary changes with experience.\nWe can make a guess at what the parameters for a and b might be. It looks like the starting salary (intercept) could be around \\(b = 60000\\) USD, with the salary increasing with each year of experience by \\(a = 1500\\) USD, effectively a 1500 USD raise per year!\n\nsalaries_scatter = sns.scatterplot(data=salaries,\n                                   x='experience',\n                                   y='salary')\n\nx = np.arange(10)  # explanatory variable: years of experience for the x-axis\na = 1400  # slope: the rate at which the salary increases per year of experience\nb = 60000  # intercept: the starting salary\ny = a*x + b  # predicted salary: predicted salary based on years of experience.\n\n\nsalaries_scatter = sns.regplot(x=x,\n                               y=y,\n                               marker=\"+\",\n                               line_kws={\"color\": \"red\"})\nsalaries_scatter.set(xlim=(-0.5, 9.5));\n\n\n\n\n\n\n\n\nNot bad, but maybe we could do a bit better\n\nsalaries_scatter = sns.scatterplot(data=salaries,\n                                   x='experience',\n                                   y='salary')\n\nx = np.arange(10)  # explanatory variable: years of experience for the x-axis\na0 = 1400  # slope 1: the rate at which the salary increases per year of experience\na1 = 1500  # slope 2: the rate at which the salary increases per year of experience\n\nb0 = 60000  # intercept 1: the starting salary\nb1 = 58000  # intercept 2: the starting salary\n\ny0 = a0*x + b0  # predicted salary 1\ny1 = a1*x + b1  # predicted salary 2\n\n# First regression line\nsalaries_scatter = sns.regplot(x=x, y=y0, marker=\"+\")\n\n# Second regression line\nsalaries_scatter = sns.regplot(x=x, \n                               y=y1, \n                               marker=\"+\", \n                               line_kws={\"color\": \"red\"})\n\nsalaries_scatter.set(xlim=(-0.5,9.5));",
    "crumbs": [
      "2 - Model Basics"
    ]
  },
  {
    "objectID": "course_content/Chapter 2 - Model Basics.html#exercises-1",
    "href": "course_content/Chapter 2 - Model Basics.html#exercises-1",
    "title": "Chapter 2 -Model Basics",
    "section": "Exercises",
    "text": "Exercises\n\nQuestion 2 Answer\n\n\nWhat values would you put for the slope (a2) and the intercept (b2)?\nsalary_scatter = sns.scatterplot(data=salaries, x=‘experience’, y=‘salary’)\nx = np.arange(10) # explanatory variable: years of experience for the x-axis a0 = 1400 # slope 1: the rate at which the salary increases per year of experience a1 = 1500 # slope 2: the rate at which the salary increases per year of experience a2 = # slope 3: “”\nb0 = 60000 # intercept 1: the starting salary b1 = 58000 # intercept 2: the starting salary b2 = # intercept 3: “”\ny0 = a0x + b # predicted salary 1 y1 = a1x + b1 # predicted salary 2 y2 = a2*x + b2 # predicted salary 3\nFirst regression line salary_scatter = sns.regplot(x=x, y=y0, marker=“+”)\nSecond regression line salary_scatter = sns.regplot(x=x, y=y1, marker=“+”, line_kws={“color”: “red”})\nThird regression line salary_scatter = sns.regplot(x=x, y=y2, marker=“+”, line_kws={“color”: “orange”})\nsalary_scatter.set(xlim=(-0.5,9.5));\n\n\n\nsns.scatterplot(data=salaries,\n                x=\"bases\",\n                y=\"raises\",\n                hue=\"department\");\n\n\n\n\n\n\n\n\n\n\n\nAs you can see by now, all three lines have slightly different parameters. It is difficult to tell by eye which line fits the data better. We could do this ourselves, by measuring the residuals, the distance from the data (actual response) to the line (predicted response), for each model, and comparing them. But this can be time-consuming, especially if our goal is to find the best model which could involve looking at more than just two models!\nLuckily python has built-in functions that will explore all the possibilities to find the ‘best’ line. In linear regression, one of the ways to define the ‘best’ line is by finding the line that minimizes the sum of squared residuals (aka sum of squares).\nCalculating the sum of squared residuals involves taking the residual distances, squaring them, and summing them. This approach is also called Ordinary Least Squares (OLS), after which the OLS function in python takes its name.",
    "crumbs": [
      "2 - Model Basics"
    ]
  },
  {
    "objectID": "course_content/Chapter 2 - Model Basics.html#model-1-continuous-variable",
    "href": "course_content/Chapter 2 - Model Basics.html#model-1-continuous-variable",
    "title": "Chapter 2 -Model Basics",
    "section": "5.1 Model 1: Continuous Variable",
    "text": "5.1 Model 1: Continuous Variable\nHypothesis: The relationship between salary and experience is the same across all departments.\nLet’s take a look at what this looks like in practice. In this case we’re interested in explaining the variation in salary using the explantory variable, experience.\nNote that the smf.ols function automatically includes the intercept in the model, so we don’t have to specify one.\n\nmodel1 = smf.ols(formula='salary ~ experience', \n                 data=salaries)\n\nresults_mod1 = model1.fit()\n\nprint(results_mod1.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 salary   R-squared:                       0.022\nModel:                            OLS   Adj. R-squared:                  0.012\nMethod:                 Least Squares   F-statistic:                     2.238\nDate:                Mon, 09 Dec 2024   Prob (F-statistic):              0.138\nTime:                        15:33:09   Log-Likelihood:                -1089.2\nNo. Observations:                 100   AIC:                             2182.\nDf Residuals:                      98   BIC:                             2188.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept   6.215e+04   2564.318     24.238      0.000    5.71e+04    6.72e+04\nexperience   683.4794    456.895      1.496      0.138    -223.214    1590.173\n==============================================================================\nOmnibus:                       94.683   Durbin-Watson:                   1.581\nProb(Omnibus):                  0.000   Jarque-Bera (JB):                8.963\nSkew:                           0.229   Prob(JB):                       0.0113\nKurtosis:                       1.607   Cond. No.                         11.2\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe summary gives us a range of diagnostic information about the model we’ve fitted, split into three tables. Here’s a quick guide on what is included:\nTop Table - model fit info.\n\nR-squared/Adj. R-squared - The proportion of the variance in the response variable (‘salary’) explained by the model. The Adj. R-squared takes into account the number of variables in the model.\nNo. of Observations (i.e. measurements of ‘salary’) and Df degrees of freedom (No. of Observations - (1 + No. of variables in the model)).\nGeneral info - date and time that model was run, type of model etc.\nModel Fit info - inc. F-statistic, AIC and BIC, Log-Likelihood. They are not meaningful on their own, but allow you to compare different models to assess the best one.\n\nMiddle Table - an important table!\n\ncoef = coefficient estimates for the intercept and explanatory variables.\nstd err = standard errors (i.e. estimate of the uncertainty) of the coefficient estimates.\nt = t-statistic for the t-test comparing whether the coefficient is different to 0.\nP&gt;|t| = p-value for the t statistics, giving significance of coefficient.\n[0.025 0.975] = 95% confidence interval around the coefficient estimate.\n\nBottom table - Diagnostics\n\nJarque-Bera, Omnibus: test normality of residuals.\nCond, No.: Condition Number, test for multicollinearity.\nDurbin-Watson: test for autocorrelation.\n\nAs you can see, there is a lot of information in the summary. Let’s focus on a few relevant values from the model in the table to focus on.\n\nThe Adjusted R^2\nIntercept coefficient\nSlope coefficient (i.e. experience in the table)\n\nWe can also get these specific parameters directly from the model object. The params attribute of the model is a dictionary, we can look at it’s different keys to access values.\nSuch as the Intercept, Slope, R-squared\n\n## Extracting the intercept\n\nprint(\"Intercept =\", results_mod1.params['Intercept'])\n\n\n## Extracting the slope\n\nprint(\"(experience) coef. =\", results_mod1.params['experience'])\n\n\n## Extracting the R-squared\n\nprint(\"R^2 =\", results_mod1.rsquared_adj)\n\nIntercept = 62154.02335773204\n(experience) coef. = 683.4793533925676\nR^2 = 0.012348448482915608\n\n\nModel interpretation\n\nIntercept: The base salary from the line of best fit was 62154.02 USD\nSlope: The rate at which salary increases with experience was estimated to be 683.47 USD per year based on the line of best fit.\nOverall fit: The model fit was poor, only explaining 1.2% of the variation in individual’s salaries.",
    "crumbs": [
      "2 - Model Basics"
    ]
  },
  {
    "objectID": "course_content/Chapter 2 - Model Basics.html#exercises-2",
    "href": "course_content/Chapter 2 - Model Basics.html#exercises-2",
    "title": "Chapter 2 -Model Basics",
    "section": "Exercises",
    "text": "Exercises\n\nQuestion 3 Answer\n\n\nWhat does the fact that most of the salaries fall outside the confidence intervals (shaded area) tell us?\n\n\n\n# The fact that most of the data points fall outside the confidence intervals (shaded area) reflects that our line of best fit does a poor job of capturing the variation in faculty salaries. The predicted value (our line of best fit) are sometimes over and underestimating faculty salaries by as much as 20,000 dollars. \n\n\n\n\nResiduals\nWe can get the residuals by substracting the observed values (actual salaries) from the predicted values (predicted salaries) from our line of best fit. This can be plotted by plotting the predicted line, the actual data/salaries.\n\nfig, ax2 = plt.subplots(figsize=(8, 5))\n\n\n# Plot\n\n## The data\nx = salaries[\"experience\"]\ny = salaries[\"salary\"]\n\n## The fitted line\n\nslope = results_mod1.params['experience']\nintercept = results_mod1.params['Intercept']\n\nxfit = np.arange(10)\nyfit = slope * xfit + intercept\n\n# plot data\nax2.scatter(x, y, marker='x', color='k')\n\n# plot line of best fit\nax2.plot(xfit, yfit, color='r')\n\n\n# plot residuals\n\n# The function below creates a vector of the x values and y values \n# for the predicted and actual data. These will be used to create the error lines.\n\ndef residual_line(x, y, slope, intercept):\n    xs = [x, x] # x values (experience) for the real and predicted salaries\n    ys = [y, (intercept + slope * x)] # y values (salary) for the real and predicted salaries.\n    return xs, ys\n\n# Creating the plot\n## Constructing the vertical error lines\n\n# extract residual lines\nxvalues, yvalues = residual_line(x, y, slope, intercept)\n\n# plot residual lines\nerrs = ax2.plot(xvalues, yvalues, color='0.5')\n\nax2.set_xlabel(\"Experience\", fontsize=12) # x-axis label\nax2.set_ylabel(\"Salary\", fontsize=12); # y-axis label\n\n\n\n\n\n\n\n\nWe can calculate the residuals to look at the actual vs predicted values and plot these on a graph\n\n# the actual salary\nactual = salaries[\"salary\"] \n\n# the predicted salary\npredicted = results_mod1.params['experience'] * salaries[\"experience\"] + results_mod1.params['Intercept']\n\n# difference between the actual and predicted salary\ndifference = actual - predicted\n\n# years of experience to go on the x-axis\nx = salaries[\"experience\"]\n\nfig, resid_plot = plt.subplots(figsize=(8,5))\n\nresid_plot.scatter(x, difference, marker='o', color='k')\nresid_plot.set_xlabel(\"Experience\", fontsize=12)\nresid_plot.set_ylabel(\"Residuals\", fontsize=12);\n\n\n\n\n\n\n\n\nSo what do these residuals mean? Well if we look at the residuals on the y-axis in some cases we are over and underestimating a professor’s salary by as much as 20,000 USD. For a large number of individuals we are over and underestimating their salraries between 10,000 and 20,000, which isn’t great. We will want to improve our estimates, although the level of error that is acceptable might depend on our stakeholders and how they are going to use this information.\nWe can then calculate the root mean squared error by taking the difference squared and then the square root.\n\n# To get the root mean squared error we need to take the difference squared and then the square root.\n\n# squaring the difference. This is also a convenient way\n# to get rid of the negative sign and look at how far our line of best fit is from the data,\n# regardless of whether the salary was under or overestimated.\ndifference_squared = difference**2\n\n# We then take the square root to get\n# back to the absolute values.\nresiduals = difference_squared.apply(math.sqrt)\n\n# collect all of the residuals in a sum\nresiduals.sum()\n\n1168916.8131155875\n\n\nResiduals\nWe can then inspect the residuals in the model. Residuals represent the left over variation in the response variable not explained by the model. A pattern in the residuals may indicate that we are missing a variable or that our assumption of normality is incorrect. When we are looking at the residuals, we want them to form an ‘amorphous cloud’, i.e. a cloud shape with no patterns.\n\nResiduals vs Fitted: is used to check the assumptions of linearity. If the residuals are spread equally around a horizontal line without distinct patterns (red line is approximately horizontal at zero), that is a good indication of having a linear relationship.\nNormal Q-Q: is used to check the normality of residuals assumption. If the majority of the residuals follow the straight dashed line, then the assumption is fulfilled.\nScale-Location: is used to check the homoscedasticity of residuals (equal variance of residuals). If the residuals are spread randomly and you see a horizontal line with equally (randomly) spread points, then the assumption is fulfilled.\nResiduals vs Leverage: is used to identify any influential value in our dataset. Influential values are extreme values that might influence the regression results when included or excluded from the analysis. Look for cases outside of a dashed line.\n\nWe will be focusing today on residuals vs. fitted and the Normal Q-Q plot.\n\nsalaries_resid = sns.residplot(data=salaries,\n                               y='salary', \n                               x='experience',\n                               scatter_kws={'alpha':0.5})\nsalaries_resid;\n\n\n\n\n\n\n\n\nWe can see from the residuals that our estimates are not great. We seem to be doing an equally poor job of under and overestimating the salaries, with slightly more salaries underestimated.\nChecking for normality of the residuals\nOne of the assumptions of a linear model is that the residuals are normally distributed. Note that this does not mean that the response variable needs to come from a normal distribution, just that the residuals (i.e. errors) are normally distributed with some below and some above with most of the residuals being only a little bit off. These values form the peak of the bell curve and are a little closer to the predicted line. Normal distributions are often referred to as bell curves because of their shape. Let’s take a look at a normal distribution with mean 0 and variance 1.\nNormal\n\n## Inspecting the normal distribution\n\nmu = 0 # the mean, often represented by the greek letter mu.\nvariance = 1 # the variance, often represented by sigma (std deviation) squared.\n\nsigma = math.sqrt(variance)\nx = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n\nplt.plot(x, stats.norm.pdf(x, mu, sigma))\nplt.xlabel('Value')\nplt.ylabel('Density')\nplt.show()\n\n\n\n\n\n\n\n\nWhat you’ll notice in the plot above is that it is symmetrical with most values falling close to 0, i.e. most values are not far off from the predicted values but could be a little above (positive) or below (negative). As you get further from the predicted value, the amount of data underneath the curve should be less (less values are way off).\nA common plot used to inspect model residuals is a qqplot. The qqplot compares the residuals to a theoretical normal distribution. If the residuals come from a normal distribution we would expect the blue dots to line up with the red line.\n\nresidual = results_mod1.resid\n\n# Use statsmodels qqplot to graph residuals\n# make a figure and an axis\nf, ax = plt.subplots(figsize=(7,7))\n\n# call the qqplot graph from statsmodels 'graphics' module.\n# fits against the normal distribution as standard.\nsm.graphics.qqplot(residual, line='45', fit=True, ax=ax);\n\n\n\n\n\n\n\n\nOur plot doesn’t look particularly good. Few of the residuals match the red line, and our qqplot plot is slightly s-shaped with more values being under and overestimated than what we would expect as part of a normal distribution. This reflects that our model is a poor fit to our data. We can see whether including department in the model helps explain some of the remaining variation in the model.",
    "crumbs": [
      "2 - Model Basics"
    ]
  },
  {
    "objectID": "course_content/Chapter 2 - Model Basics.html#model-2-continuous-and-categorical-variables",
    "href": "course_content/Chapter 2 - Model Basics.html#model-2-continuous-and-categorical-variables",
    "title": "Chapter 2 -Model Basics",
    "section": "6.1 Model 2: Continuous and Categorical Variables",
    "text": "6.1 Model 2: Continuous and Categorical Variables\nHypothesis: Each department has a different starting salary but the increase in salary with experience is the same.\nNow we are constructing our model to explain the variation in salary using the variables for experience and department. In this case, because department is a categorical variable we will want to specify it using C(department).\n\nmodel2 = smf.ols(formula='salary ~ experience + C(department)',\n                 data=salaries)\n\nresults_mod2 = model2.fit()\n\nprint(results_mod2.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 salary   R-squared:                       0.922\nModel:                            OLS   Adj. R-squared:                  0.918\nMethod:                 Least Squares   F-statistic:                     222.0\nDate:                Mon, 09 Dec 2024   Prob (F-statistic):           1.97e-50\nTime:                        15:33:11   Log-Likelihood:                -962.82\nNo. Observations:                 100   AIC:                             1938.\nDf Residuals:                      94   BIC:                             1953.\nDf Model:                           5                                         \nCovariance Type:            nonrobust                                         \n================================================================================================\n                                   coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------------------------\nIntercept                     4.973e+04   1026.409     48.454      0.000    4.77e+04    5.18e+04\nC(department)[T.english]      9246.4336   1198.623      7.714      0.000    6866.539    1.16e+04\nC(department)[T.informatics]  2.422e+04   1199.285     20.193      0.000    2.18e+04    2.66e+04\nC(department)[T.sociology]   -1933.5688   1215.500     -1.591      0.115   -4346.973     479.835\nC(department)[T.statistics]   2.883e+04   1204.055     23.948      0.000    2.64e+04    3.12e+04\nexperience                     755.5152    134.641      5.611      0.000     488.183    1022.847\n==============================================================================\nOmnibus:                        1.702   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.427   Jarque-Bera (JB):                1.457\nSkew:                          -0.143   Prob(JB):                        0.483\nKurtosis:                       2.482   Cond. No.                         30.7\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThere are a few key things to pick out from the table above. First, we have an increase in the adjusted R^2, which is now 0.922, meaning we’ve explained 92.2 % of the variation in salary. We also have several parameter estimates now:\n\nIntercept\nC(department)[T.english]\nC(department)[T.informatics]\nC(department)[T.sociology]\nC(department)[T.statistics]\nexperience\n\nBut remember that in our salaries dataset there are five departments at the University:\n\nbiology\nenglish\ninformatics\nsociology\nstatistics\n\nCategorical variables always contribute to the intercept in a linear model. When fitting the model, the model function uses the first level of the categorical variable, in this case biology, as the baseline Intercept. The coefficients estimated for C(department)[T.english] and C(department)[T.informatics], and others are estimated in relation to that.\nLet’s see how our fit looks when it is plotted.\n\nscatter_salaries = sns.scatterplot(data=salaries, \n                                   x='experience', \n                                   y='salary', \n                                   hue='department')\n\nx = np.arange(10)\na = results_mod2.params['experience']\n\nb1 = results_mod2.params['Intercept'] # biology\nb2 = results_mod2.params['C(department)[T.english]'] + results_mod2.params['Intercept'] # english\nb3 = results_mod2.params['C(department)[T.informatics]']  + results_mod2.params['Intercept'] # informatics\nb4 = results_mod2.params['C(department)[T.sociology]'] + results_mod2.params['Intercept'] # sociology\nb5 = results_mod2.params['C(department)[T.statistics]']  + results_mod2.params['Intercept'] # statistics\n\ny1= a*x + b1 # biology predicted salaries\ny2 = a*x + b2 # economics \"\"\ny3 = a*x + b3 # informationcs \"\"\ny4 = a*x + b4 # sociology \"\"\ny5 = a*x + b5 # statistics \"\"\n\n# Biology\nscatter_salaries = sns.regplot(x=x, \n                               y=y1, \n                               marker=\"+\", \n                               line_kws={\"color\": \"orange\"})\n# Economics\nscatter_salaries = sns.regplot(x=x, \n                               y=y2, \n                               marker=\"+\", \n                               line_kws={\"color\": \"green\"})\n# Informatics\nscatter_salaries = sns.regplot(x=x, \n                               y=y3, \n                               marker=\"+\", \n                               line_kws={\"color\": \"red\"})\n# Sociology\nscatter_salaries = sns.regplot(x=x, \n                               y=y4, \n                               marker=\"+\", \n                               line_kws={\"color\": \"blue\"})\n# Statistics\nscatter_salaries = sns.regplot(x=x, \n                               y=y5, \n                               marker=\"+\", \n                               line_kws={\"color\": \"purple\"})\n\n\n\nscatter_salaries.set(xlim=(-0.5,9.5))\nplt.legend(loc='upper right');\n\n\n\n\n\n\n\n\nAs you can see from the plot above, all of the lines have the same slope so that they are parallel to one another. The only thing that changes is the intercept which moves the fitted line up or down.\nLet’s see what the residuals look like for our new model.\n\n## Residuals for Model 2\n\nresidual2 = results_mod2.resid\nfitted2 = results_mod2.fittedvalues\n\nresid2_plot = sns.scatterplot(x = fitted2, \n                              y = residual2)\nresid2_plot.set(xlabel='Fitted', \n                ylabel='Residuals')\nresid2_plot;\n\n## Extracting the intercept\n\nprint(\"Intercept =\", (results_mod2.params['C(department)[T.sociology]'] + results_mod2.params['Intercept']))\n\n\n## Extracting the slope\n\nprint(\"(experience) coef. =\", results_mod2.params['experience'])\n\n\n## Extracting the R-squared\n\nprint(\"R^2 =\", results_mod2.rsquared_adj)\n\nIntercept = 47800.20186457718\n(experience) coef. = 755.5152142855145\nR^2 = 0.9177865863237862\n\n\n\n\n\n\n\n\n\nLet’s inspect the qqplot\n\n# Use statsmodels qqplot to graph errors\n# make a figure and an axis\nf, ax = plt.subplots(figsize=(7,7))\n# call the qqplot graph from statsmodels 'graphics' module.\n# fits against the normal distribution as standard.\n\nsm.graphics.qqplot(residual2, line='45', fit=True, ax=ax);\n\n\n\n\n\n\n\n\nOur residuals are looking better. There are some tails at the end, but overall the blue dots are following the red line.\nModel interpretation\nInterpret the model for the Sociology department (similar to the example in Model 1).\n\nIntercept:\nSlope:\nOverall fit:\n\n\n## Extracting the intercept\n\nprint(\"Intercept =\", (results_mod2.params['C(department)[T.sociology]'] + results_mod2.params['Intercept']))\n\n\n## Extracting the slope\n\nprint(\"(experience) coef. =\", results_mod2.params['experience'])\n\n\n## Extracting the R-squared\n\nprint(\"R^2 =\", results_mod2.rsquared_adj)\n\n\n\n# Model interpretation\n\n## Intercept: The base salary for the sociology department from the line of best fit was 47800.2 USD.\n## Slope: The rate at which salary increases with experience was estimated to be 755.5 USD per year based on the line of best fit.\n## Overall fit: The model fit explained 91.8% of the variation in individual's salaries.\n\nIntercept = 47800.20186457718\n(experience) coef. = 755.5152142855145\nR^2 = 0.9177865863237862",
    "crumbs": [
      "2 - Model Basics"
    ]
  },
  {
    "objectID": "course_content/Chapter 2 - Model Basics.html#model-3-interactions",
    "href": "course_content/Chapter 2 - Model Basics.html#model-3-interactions",
    "title": "Chapter 2 -Model Basics",
    "section": "6.2 Model 3: Interactions",
    "text": "6.2 Model 3: Interactions\nHypothesis: Each department has different starting salaries and salaries increase at different rates.\nSo far we’ve allowed the intercept in our model to vary by department. Let’s try letting our slope change also with department. By allowing our slope to vary by department we are exploring the hypothesis that how salary changes with experience varies by department.\nTo do this in the model, we need to use an interaction term. Interactions are included using the multiplication symbol \\(*\\) between two variables.\n\n## Specifying Model 3 and the resulting model fit\n\nmodel3 = smf.ols(formula='salary ~ experience*C(department)', data=salaries)\n\nresults_mod3 = model3.fit()\n\nprint(results_mod3.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 salary   R-squared:                       0.941\nModel:                            OLS   Adj. R-squared:                  0.935\nMethod:                 Least Squares   F-statistic:                     159.2\nDate:                Mon, 09 Dec 2024   Prob (F-statistic):           2.65e-51\nTime:                        15:33:12   Log-Likelihood:                -948.90\nNo. Observations:                 100   AIC:                             1918.\nDf Residuals:                      90   BIC:                             1944.\nDf Model:                           9                                         \nCovariance Type:            nonrobust                                         \n===========================================================================================================\n                                              coef    std err          t      P&gt;|t|      [0.025      0.975]\n-----------------------------------------------------------------------------------------------------------\nIntercept                                 5.18e+04   1321.559     39.197      0.000    4.92e+04    5.44e+04\nC(department)[T.english]                 9055.8441   1884.747      4.805      0.000    5311.466    1.28e+04\nC(department)[T.informatics]             1.861e+04   1982.352      9.386      0.000    1.47e+04    2.25e+04\nC(department)[T.sociology]              -1.088e+04   2483.215     -4.380      0.000   -1.58e+04   -5944.093\nC(department)[T.statistics]              2.865e+04   1953.622     14.666      0.000    2.48e+04    3.25e+04\nexperience                                274.7322    252.471      1.088      0.279    -226.845     776.310\nexperience:C(department)[T.english]        39.1883    363.691      0.108      0.914    -683.347     761.724\nexperience:C(department)[T.informatics]  1251.3258    374.269      3.343      0.001     507.775    1994.876\nexperience:C(department)[T.sociology]    1666.3859    422.191      3.947      0.000     827.630    2505.142\nexperience:C(department)[T.statistics]    114.7960    346.958      0.331      0.742    -574.496     804.088\n==============================================================================\nOmnibus:                        2.239   Durbin-Watson:                   1.858\nProb(Omnibus):                  0.326   Jarque-Bera (JB):                1.557\nSkew:                          -0.052   Prob(JB):                        0.459\nKurtosis:                       2.397   Cond. No.                         61.7\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nIf we look at the results, we can see that the slope for experience also uses biology (the department name that is missing) as a baseline.\nWe can plot the model fit using our new coefficients. This time, both the slope and intercept will vary.\n\nscatter_department = sns.scatterplot(data=salaries, x='experience', y='salary', hue='department')\n\nx = np.arange(10)\n\n##Slope\na1 = results_mod3.params['experience']\na2 = results_mod3.params['experience:C(department)[T.english]'] + results_mod3.params['experience']\na3 = results_mod3.params['experience:C(department)[T.informatics]'] + results_mod3.params['experience']\na4 = results_mod3.params['experience:C(department)[T.sociology]'] + results_mod3.params['experience']\na5 = results_mod3.params['experience:C(department)[T.statistics]'] + results_mod3.params['experience']\n\nprint(a1, a2, a3, a4, a5)\n\n## Intercept\nb1 = results_mod3.params['Intercept']\nb2 = results_mod3.params['C(department)[T.english]'] + results_mod3.params['Intercept']\nb3 = results_mod3.params['C(department)[T.informatics]'] + results_mod3.params['Intercept']\nb4 = results_mod3.params['C(department)[T.sociology]'] + results_mod3.params['Intercept']\nb5 = results_mod3.params['C(department)[T.statistics]'] + results_mod3.params['Intercept']\n\nprint(b1, b2, b3, b4, b5)\n\n##Predicted Salary\ny1 = a1*x + b1\ny2 = a2*x + b2\ny3 = a3*x + b3\ny4 = a4*x + b4\ny5 = a5*x + b5\n\n\n## Line of best fit\n\nscatter_department = sns.regplot(x = x, \n                                 y = y1, \n                                 marker= \"+\", \n                                 line_kws = {\"color\": \"orange\"})\n\nscatter_department = sns.regplot(x = x, \n                                 y = y2, \n                                 marker = \"+\", \n                                 line_kws = {\"color\": \"green\"})\n\nscatter_department = sns.regplot(x = x, \n                                 y = y3, \n                                 marker = \"+\", \n                                 line_kws = {\"color\": \"red\"})\n\nscatter_department = sns.regplot(x = x, \n                                 y = y4, \n                                 marker = \"+\", \n                                 line_kws = {\"color\": \"blue\"})\n\nscatter_department = sns.regplot(x = x, \n                                 y = y5, \n                                 marker = \"+\", \n                                 line_kws = {\"color\": \"purple\"})\n\nscatter_department.set_xlim(left=-0.5, right=12)\nplt.legend(loc='upper right');\n\n274.7321674531986 313.92050330987126 1526.0579500530607 1941.1180412513213 389.52819245807586\n51801.13777457291 60856.9818289405 70406.95435976752 40923.705468175394 80453.2579141881",
    "crumbs": [
      "2 - Model Basics"
    ]
  },
  {
    "objectID": "course_content/Chapter 2 - Model Basics.html#exercises-3",
    "href": "course_content/Chapter 2 - Model Basics.html#exercises-3",
    "title": "Chapter 2 -Model Basics",
    "section": "Exercises",
    "text": "Exercises\n\nQuestion 4 Answers\n\n\n\nWhich department gets the highest raises (biggest slope)?\nWhich department has the highest starting salary (biggest intercept)?\n\n\n\n\n## 1. Which department gets the highest raises (biggest slope)?\n\n\n# Looking at the plot both the informatics department and the sociology department have steep positive slopes but it is difficult to tell by eye which has a higher slope (estimated yearly raise).\n\na3 = results_mod3.params['experience:C(department)[T.informatics]'] + results_mod3.params['experience']\na4 = results_mod3.params['experience:C(department)[T.sociology]'] + results_mod3.params['experience']\n\nprint('Informatics slope:', a3)\nprint('Sociology slope:', a4)\n\n## Yearly raise: The sociology department has the highest estimated yearly raise with 1941.11 USD increase in pay per year. The informatics department has the second highest estimates yearly raise with 1526.05 USD increase in pay per year.\n\n## 2. Which department has the highest starting salary (biggest intercept)?\n\n# Looking at the plot the statistics department has the largest intercept (highest point crossing the y-axis). \n\nb5 = results_mod3.params['C(department)[T.statistics]'] + results_mod3.params['Intercept']\n\nprint('Statistics Intercept', b5)\n\n\n## The starting salary is estimated at 80,453.3 USD.\n\nInformatics slope: 1526.0579500530607\nSociology slope: 1941.1180412513213\nStatistics Intercept 80453.2579141881\n\n\n\n\n\nModel interpretation\nInterpret the model for the Informatics department (similar to the example in Model 1).\n\nIntercept:\nSlope:\nOverall fit:",
    "crumbs": [
      "2 - Model Basics"
    ]
  },
  {
    "objectID": "course_content/Chapter 2 - Model Basics.html#making-predictions-from-our-model",
    "href": "course_content/Chapter 2 - Model Basics.html#making-predictions-from-our-model",
    "title": "Chapter 2 -Model Basics",
    "section": "6.3 Making Predictions from our model",
    "text": "6.3 Making Predictions from our model\nNow that we’ve settled on our best model, let’s say that someone asked us the question:\nWhat would be the salary of a professor who has worked in the biology department for 7 years?\n\n# We can calculate this by plugging in 7 years into our model equation for biology\n# (remember: biology is incorporated into the intercept)\n\nresults_mod3.params['experience']*7 + results_mod3.params['Intercept']\n\n53724.2629467453\n\n\nNow if someone asks us what would be the salary of a professor who has worked in the sociology department for 7 years we can approach it in the same way, but it will be a bit longer as our equations are in reference to a biology baseline.\n\n# We can calculate this by plugging in 7 years into our model equation for sociology\n# Note that we have to combine the baseline slope with our slope for sociology\n# We also have to combine our baseline intercept with our intercept for sociology\n\n(results_mod3.params['experience'] + results_mod3.params['experience:C(department)[T.sociology]'])*7 + results_mod3.params['Intercept'] + results_mod3.params['C(department)[T.sociology]']\n\n54511.531756934644",
    "crumbs": [
      "2 - Model Basics"
    ]
  },
  {
    "objectID": "course_content/Chapter 2 - Model Basics.html#exercises-4",
    "href": "course_content/Chapter 2 - Model Basics.html#exercises-4",
    "title": "Chapter 2 -Model Basics",
    "section": "Exercises",
    "text": "Exercises\n\nQuestion 5 Answer\n\n\nWhat is the predicted salary of someone who works in the informatics department for 7 years?\n\n\n\n# We can calculate this by plugging in 7 years into our model equation for informatics\n# Note that we have to combine the baseline slope with our slope for informatics\n# We also have to combine our baseline intercept with our intercept for informatics\n\n(results_mod3.params['experience'] + results_mod3.params['experience:C(department)[T.informatics]'])*7 + results_mod3.params['Intercept'] + results_mod3.params['C(department)[T.informatics]']\n\n81089.36001013895\n\n\n\n\n\nExtrapolation\nWhen we make predictions for the salary, we want to make sure that we are not extrapolating beyond the data that we have observed. For instance, we can’t with any confidence say what a professors salary will be in 20 years because we do not have any professors in our data set who have worked in a department for 20 years. It’s important not to extrapolate in areas we have not measure or don’t have data for because we don’t know how our model will perform in that sample space.\nResiduals Model 3\n\nresidual3 = results_mod3.resid\nfitted3 = results_mod3.fittedvalues\n\nresid3_plot = sns.scatterplot(x = fitted3, y = residual3)\nresid3_plot.set(xlabel='Fitted', ylabel='Residuals');\n\n\n\n\n\n\n\n\nThe qqplot for Model 3\n\n# Use statsmodels qqplot to graph errors\n# make a figure and an axis\nf, ax = plt.subplots(figsize=(7,7))\n# call the qqplot graph from statsmodels 'graphics' module.\n# fits against the normal distribution as standard.\n\nsm.graphics.qqplot(residual3, line='45', fit=True, ax=ax);",
    "crumbs": [
      "2 - Model Basics"
    ]
  },
  {
    "objectID": "course_content/Chapter 2 - Model Basics.html#exercises-5",
    "href": "course_content/Chapter 2 - Model Basics.html#exercises-5",
    "title": "Chapter 2 -Model Basics",
    "section": "Exercises",
    "text": "Exercises\n\nQuestion 6 Answer\n\n\nCompare the models we ran using Adjusted R^2 and AIC. Using the notes above, discuss in groups or write a paragraph about which model you think is best and why?  Take a look at the main parameters of Model 2 and Model 3 from the model summary tables. Do they seem to vary much between the models?\n\n\n\n# 1. Let's compare the models we ran using Adjusted R^2 and AIC. Using the notes above, discuss in groups or write a paragraph about which model you think is best and why?\n\nprint(\"Adjusted R^2 Model 1 = \", results_mod1.rsquared_adj, \n      \"\\nAdjusted R^2 Model 2 = \", results_mod2.rsquared_adj, \n      \"\\nAdjusted R^2 Model 3 = \", results_mod3.rsquared_adj)\n\n## Model 3 explains the most variation out of the models with 93.5% of the variation in salary explained compared to 91.8% of variation in salary explained by Model 2.  Model 3 also has the lowest AIC 1917.8 compared to 1937.6 for Model 2. \n\nprint(\"AIC Model 1 = \", results_mod1.aic, \n      \"\\nAIC Model 2 = \", results_mod2.aic, \n      \"\\nAIC Model 3 = \", results_mod3.aic)\n\n#2. Take a look at the main parameters of Model 2 and Model 3 from the model summary tables. Do they seem to vary much between the models?\n\n## Model 2\n\nb1 = results_mod2.params['Intercept'] # biology\nb2 = results_mod2.params['C(department)[T.english]'] + results_mod2.params['Intercept'] # english\nb3 = results_mod2.params['C(department)[T.informatics]']  + results_mod2.params['Intercept'] # informatics\nb4 = results_mod2.params['C(department)[T.sociology]'] + results_mod2.params['Intercept'] # sociology\nb5 = results_mod2.params['C(department)[T.statistics]']  + results_mod2.params['Intercept'] # statistics\n\nprint(b1, b2, b3, b4, b5)\n\n\n## The starting salary (intercept) ranges from 47800.20-78568.42 USD. \n## The yearly raise (slope) is the same for all departments and is 755.52 USD.\n\n\n## Model 3\n\n##Slope\na1 = results_mod3.params['experience']\na2 = results_mod3.params['experience:C(department)[T.english]'] + results_mod3.params['experience']\na3 = results_mod3.params['experience:C(department)[T.informatics]'] + results_mod3.params['experience']\na4 = results_mod3.params['experience:C(department)[T.sociology]'] + results_mod3.params['experience']\na5 = results_mod3.params['experience:C(department)[T.statistics]'] + results_mod3.params['experience']\n\nprint(a1, a2, a3, a4, a5)\n\n## The slope (yearly raises) varies between 274.73 and 1941.12 USD per year.\n\n\n## Intercept\nb1 = results_mod3.params['Intercept']\nb2 = results_mod3.params['C(department)[T.english]'] + results_mod3.params['Intercept']\nb3 = results_mod3.params['C(department)[T.informatics]'] + results_mod3.params['Intercept']\nb4 = results_mod3.params['C(department)[T.sociology]'] + results_mod3.params['Intercept']\nb5 = results_mod3.params['C(department)[T.statistics]'] + results_mod3.params['Intercept']\n\nprint(b1, b2, b3, b4, b5)\n\n## The intercept (starting salary) varies between 40923.71 and 80453.26 USD per year. \n\n\n## Comparing the two models the intercepts (starting salary) ranges do not vary greatly, however, the yearly raises vary quite a lot between Model 3 and Model 2. If we were interested in comparing department starting salaries between the model we could subtract the intercept estimated in model 3 by the intercept estimated in model 2 to see the difference. \n\n## The starting salary in biology was 2067.37 higher in model 3 compared to model 2\n\nprint(results_mod3.params['Intercept'] - results_mod2.params['Intercept'])\n\n## The yearly raise in biology was estimated to be 480.78 USD per year lower in model 3 compared to model 2\n\nprint(results_mod3.params['experience']-results_mod2.params['experience'])\n\nAdjusted R^2 Model 1 =  0.012348448482915608 \nAdjusted R^2 Model 2 =  0.9177865863237862 \nAdjusted R^2 Model 3 =  0.9349987583834514\nAIC Model 1 =  2182.409691980974 \nAIC Model 2 =  1937.6412740676572 \nAIC Model 3 =  1917.801552808499\n49733.77067319402 58980.20430729411 73951.45094429822 47800.20186457718 78568.42475177687\n274.7321674531986 313.92050330987126 1526.0579500530607 1941.1180412513213 389.52819245807586\n51801.13777457291 60856.9818289405 70406.95435976752 40923.705468175394 80453.2579141881\n2067.3671013788917\n-480.7830468323159\n\n\n\n\n\nAdjusted R^2\n\n## Model Comparison: Comparing Adjusted R^2\n## \\n will put the results on the next line!\n\nprint(\"Adjusted R^2 Model 1 = \", results_mod1.rsquared_adj, \n      \"\\nAdjusted R^2 Model 2 = \", results_mod2.rsquared_adj, \n      \"\\nAdjusted R^2 Model 3 = \", results_mod3.rsquared_adj)\n\nAdjusted R^2 Model 1 =  0.012348448482915608 \nAdjusted R^2 Model 2 =  0.9177865863237862 \nAdjusted R^2 Model 3 =  0.9349987583834514\n\n\nAIC\n\nprint(\"AIC Model 1 = \", results_mod1.aic, \n      \"\\nAIC Model 2 = \", results_mod2.aic, \n      \"\\nAIC Model 3 = \", results_mod3.aic)\n\nAIC Model 1 =  2182.409691980974 \nAIC Model 2 =  1937.6412740676572 \nAIC Model 3 =  1917.801552808499\n\n\nIn the next chapter we will extend the modelling basics we have learned here into more advanced linear model types.",
    "crumbs": [
      "2 - Model Basics"
    ]
  },
  {
    "objectID": "datasets.html",
    "href": "datasets.html",
    "title": "Datasets",
    "section": "",
    "text": "Below are the list of datasets used throughout the course. Click on the links to download the data files:\nDownload Brook_Bridge_Bicycle_Counts.csv\nDownload faculty-data.csv",
    "crumbs": [
      "Datasets"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics in Python",
    "section": "",
    "text": "Welcome to the Statistics in Python course",
    "crumbs": [
      "Course Information"
    ]
  },
  {
    "objectID": "index.html#general-information",
    "href": "index.html#general-information",
    "title": "Statistics in Python",
    "section": "General Information",
    "text": "General Information\nThis course introduces the basics of carrying out a statistical analysis in Python. It covers exploratory data analysis and constructing and interpreting linear and generalized linear models. Each chapter builds on the previous one, introducing progressively advanced topics while ensuring practical hands-on experience with relevant Python packages.",
    "crumbs": [
      "Course Information"
    ]
  },
  {
    "objectID": "index.html#course-materials",
    "href": "index.html#course-materials",
    "title": "Statistics in Python",
    "section": "Course Materials",
    "text": "Course Materials\nThe course materials come in several formats:\n\nHTML pages such as the one you are reading now\nData  we will use during the course. It’s highly recommended you create a project with a ‘data’ folder and download all the required datasets before starting the course\n\nYou can also navigate to the course Github Repository and clone or fork the website structure for yourself. If you are new to programming and version control, we recommend you remain on the website to gain the best experience.",
    "crumbs": [
      "Course Information"
    ]
  },
  {
    "objectID": "index.html#software-requirements",
    "href": "index.html#software-requirements",
    "title": "Statistics in Python",
    "section": "Software Requirements",
    "text": "Software Requirements\n\nPython (Version 3.7 or higher)\nAnaconda\nThe main packages we will be using for this course are: - matplotlib==3.3.4  - pandas==1.1.5  - scikit-learn==0.24.2  - seaborn==0.11.1  - statsmodels==0.12.2",
    "crumbs": [
      "Course Information"
    ]
  },
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "Statistics in Python",
    "section": "Course Overview",
    "text": "Course Overview\n\nChapter 1: Exploratory Data Analysis\nIn this chapter, we will delve into the principles of tidy data and explore the concepts of variables, values, and observations. Learners will use Python to analyze the structure of datasets and differentiate between continuous and categorical variables. We’ll examine the significance of variation and covariation in data and explore their roles within Exploratory Data Analysis (EDA). Visual tools will be leveraged to uncover patterns and relationships in data, enhancing our understanding of variable interactions.\n\n\nChapter 2 - Model Basics\nThis chapter introduces the foundations of statistical modeling. Learners will explore model families, fitted models, and the differences between response and explanatory variables. The process of constructing linear models in Python will be covered, with explanations of key components like slopes and intercepts. Learners will practice extracting parameters from model objects and interpreting tables generated by fitted models. Techniques for assessing model fit, including the use of residuals, Adjusted R-squared, and AIC for model comparison, will also be discussed.\n\n\nChapter 3 - Generalized Linear Models\nBuilding on the basics of linear modeling, this chapter introduces generalized linear models (GLMs). Learners will explore fundamental probability concepts, including random variables and probability distributions, with a focus on their application to real-world data. Common probability distributions, such as binomial, normal, Poisson, and negative binomial, will be discussed. Finally, we will implement generalized linear models in Python, demonstrating their flexibility in handling various types of response variables.",
    "crumbs": [
      "Course Information"
    ]
  }
]